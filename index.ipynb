{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df=pd.read_csv('Bank_complaints.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation fo label encoder\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)  \n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, \n",
    "                                                    product_onehot, test_size=1500, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0703 14:27:11.744946  2256 deprecation_wrapper.py:119] From C:\\Users\\kylej\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0703 14:27:11.756914  2256 deprecation_wrapper.py:119] From C:\\Users\\kylej\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0703 14:27:11.758914  2256 deprecation_wrapper.py:119] From C:\\Users\\kylej\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "mod = models.Sequential()\n",
    "mod.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "mod.add(layers.Dense(25, activation='relu'))\n",
    "mod.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0703 14:27:47.089732  2256 deprecation_wrapper.py:119] From C:\\Users\\kylej\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0703 14:27:47.101732  2256 deprecation_wrapper.py:119] From C:\\Users\\kylej\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0703 14:28:12.423040  2256 deprecation.py:323] From C:\\Users\\kylej\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0703 14:28:12.512037  2256 deprecation_wrapper.py:119] From C:\\Users\\kylej\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9488 - acc: 0.1532 - val_loss: 1.9361 - val_acc: 0.1710\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9333 - acc: 0.1660 - val_loss: 1.9236 - val_acc: 0.1740\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.9208 - acc: 0.1829 - val_loss: 1.9123 - val_acc: 0.1870\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9085 - acc: 0.2043 - val_loss: 1.9001 - val_acc: 0.2130\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8948 - acc: 0.2227 - val_loss: 1.8862 - val_acc: 0.2340\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8788 - acc: 0.2471 - val_loss: 1.8695 - val_acc: 0.2560\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8597 - acc: 0.2679 - val_loss: 1.8492 - val_acc: 0.2860\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8371 - acc: 0.2939 - val_loss: 1.8259 - val_acc: 0.3020\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8111 - acc: 0.3177 - val_loss: 1.7986 - val_acc: 0.3280\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7809 - acc: 0.3401 - val_loss: 1.7665 - val_acc: 0.3620\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7475 - acc: 0.3700 - val_loss: 1.7318 - val_acc: 0.3870\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7110 - acc: 0.4003 - val_loss: 1.6937 - val_acc: 0.4060\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6719 - acc: 0.4208 - val_loss: 1.6545 - val_acc: 0.4230\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6316 - acc: 0.4387 - val_loss: 1.6127 - val_acc: 0.4500\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5896 - acc: 0.4584 - val_loss: 1.5701 - val_acc: 0.4750\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5472 - acc: 0.4796 - val_loss: 1.5284 - val_acc: 0.4910\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5043 - acc: 0.5035 - val_loss: 1.4852 - val_acc: 0.5190\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4612 - acc: 0.5231 - val_loss: 1.4436 - val_acc: 0.5310\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4182 - acc: 0.5421 - val_loss: 1.4007 - val_acc: 0.5610\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3751 - acc: 0.5649 - val_loss: 1.3585 - val_acc: 0.5860\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3324 - acc: 0.5813 - val_loss: 1.3172 - val_acc: 0.6070\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2907 - acc: 0.5995 - val_loss: 1.2783 - val_acc: 0.6220\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2494 - acc: 0.6176 - val_loss: 1.2386 - val_acc: 0.6330\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2097 - acc: 0.6271 - val_loss: 1.2025 - val_acc: 0.6430\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1714 - acc: 0.6416 - val_loss: 1.1670 - val_acc: 0.6500\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1348 - acc: 0.6525 - val_loss: 1.1325 - val_acc: 0.6620\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1001 - acc: 0.6579 - val_loss: 1.1001 - val_acc: 0.6760\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0670 - acc: 0.6684 - val_loss: 1.0705 - val_acc: 0.6760\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0358 - acc: 0.6783 - val_loss: 1.0421 - val_acc: 0.6800\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0064 - acc: 0.6839 - val_loss: 1.0141 - val_acc: 0.6900\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9789 - acc: 0.6905 - val_loss: 0.9915 - val_acc: 0.6960\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9525 - acc: 0.6984 - val_loss: 0.9670 - val_acc: 0.6950\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9284 - acc: 0.7032 - val_loss: 0.9447 - val_acc: 0.6970\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9054 - acc: 0.7092 - val_loss: 0.9242 - val_acc: 0.7000\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8841 - acc: 0.7131 - val_loss: 0.9058 - val_acc: 0.7040\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8637 - acc: 0.7172 - val_loss: 0.8885 - val_acc: 0.7000\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8452 - acc: 0.7228 - val_loss: 0.8723 - val_acc: 0.7110\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8278 - acc: 0.7259 - val_loss: 0.8567 - val_acc: 0.7080\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8112 - acc: 0.7324 - val_loss: 0.8431 - val_acc: 0.7080\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7957 - acc: 0.7345 - val_loss: 0.8292 - val_acc: 0.7150\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7807 - acc: 0.7360 - val_loss: 0.8185 - val_acc: 0.7170\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7671 - acc: 0.7407 - val_loss: 0.8068 - val_acc: 0.7150\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7541 - acc: 0.7431 - val_loss: 0.7969 - val_acc: 0.7160\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7417 - acc: 0.7457 - val_loss: 0.7881 - val_acc: 0.7230\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7302 - acc: 0.7484 - val_loss: 0.7800 - val_acc: 0.7210\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7191 - acc: 0.7516 - val_loss: 0.7725 - val_acc: 0.7240\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7091 - acc: 0.7556 - val_loss: 0.7625 - val_acc: 0.7250\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6990 - acc: 0.7571 - val_loss: 0.7538 - val_acc: 0.7320\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6896 - acc: 0.7603 - val_loss: 0.7485 - val_acc: 0.7300\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6808 - acc: 0.7609 - val_loss: 0.7412 - val_acc: 0.7340\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6719 - acc: 0.7623 - val_loss: 0.7360 - val_acc: 0.7320\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6638 - acc: 0.7663 - val_loss: 0.7294 - val_acc: 0.7300\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6559 - acc: 0.7663 - val_loss: 0.7254 - val_acc: 0.7380\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6484 - acc: 0.7697 - val_loss: 0.7211 - val_acc: 0.7360\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6413 - acc: 0.7729 - val_loss: 0.7158 - val_acc: 0.7380\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6341 - acc: 0.7748 - val_loss: 0.7117 - val_acc: 0.7370\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6272 - acc: 0.7757 - val_loss: 0.7060 - val_acc: 0.7430\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6209 - acc: 0.7776 - val_loss: 0.7049 - val_acc: 0.7480\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6150 - acc: 0.7805 - val_loss: 0.6993 - val_acc: 0.7480\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6088 - acc: 0.7809 - val_loss: 0.6991 - val_acc: 0.7500\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6030 - acc: 0.7849 - val_loss: 0.6936 - val_acc: 0.7400\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5971 - acc: 0.7867 - val_loss: 0.6900 - val_acc: 0.7460\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5917 - acc: 0.7888 - val_loss: 0.6870 - val_acc: 0.7510\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5861 - acc: 0.7892 - val_loss: 0.6843 - val_acc: 0.7440\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5808 - acc: 0.7940 - val_loss: 0.6807 - val_acc: 0.7480\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5759 - acc: 0.7937 - val_loss: 0.6807 - val_acc: 0.7470\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5712 - acc: 0.7955 - val_loss: 0.6796 - val_acc: 0.7460\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5664 - acc: 0.7977 - val_loss: 0.6759 - val_acc: 0.7460\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5613 - acc: 0.7989 - val_loss: 0.6731 - val_acc: 0.7420\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5567 - acc: 0.8015 - val_loss: 0.6708 - val_acc: 0.7460\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5525 - acc: 0.8019 - val_loss: 0.6683 - val_acc: 0.7510\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5479 - acc: 0.8036 - val_loss: 0.6675 - val_acc: 0.7490\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5433 - acc: 0.8065 - val_loss: 0.6663 - val_acc: 0.7490\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5393 - acc: 0.8068 - val_loss: 0.6656 - val_acc: 0.7570\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5350 - acc: 0.8072 - val_loss: 0.6643 - val_acc: 0.7540\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5313 - acc: 0.8101 - val_loss: 0.6598 - val_acc: 0.7500\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5269 - acc: 0.8107 - val_loss: 0.6591 - val_acc: 0.7550\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5235 - acc: 0.8133 - val_loss: 0.6580 - val_acc: 0.7540\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5192 - acc: 0.8147 - val_loss: 0.6575 - val_acc: 0.7460\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5152 - acc: 0.8151 - val_loss: 0.6573 - val_acc: 0.7460\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5117 - acc: 0.8149 - val_loss: 0.6562 - val_acc: 0.7520\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5081 - acc: 0.8189 - val_loss: 0.6540 - val_acc: 0.7480\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5049 - acc: 0.8192 - val_loss: 0.6536 - val_acc: 0.7520\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5006 - acc: 0.8224 - val_loss: 0.6548 - val_acc: 0.7460\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4978 - acc: 0.8220 - val_loss: 0.6516 - val_acc: 0.7500\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4939 - acc: 0.8224 - val_loss: 0.6496 - val_acc: 0.7500\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.4906 - acc: 0.8235 - val_loss: 0.6491 - val_acc: 0.7560\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4873 - acc: 0.8257 - val_loss: 0.6478 - val_acc: 0.7530\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.4842 - acc: 0.8260 - val_loss: 0.6476 - val_acc: 0.7560\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4804 - acc: 0.8283 - val_loss: 0.6474 - val_acc: 0.7530\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4772 - acc: 0.8276 - val_loss: 0.6466 - val_acc: 0.7570\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4743 - acc: 0.8299 - val_loss: 0.6449 - val_acc: 0.7540\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4710 - acc: 0.8311 - val_loss: 0.6454 - val_acc: 0.7550\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4678 - acc: 0.8324 - val_loss: 0.6448 - val_acc: 0.7570\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4648 - acc: 0.8335 - val_loss: 0.6452 - val_acc: 0.7560\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4620 - acc: 0.8331 - val_loss: 0.6436 - val_acc: 0.7580\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4591 - acc: 0.8348 - val_loss: 0.6437 - val_acc: 0.7560\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4559 - acc: 0.8373 - val_loss: 0.6434 - val_acc: 0.7530\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4529 - acc: 0.8379 - val_loss: 0.6441 - val_acc: 0.7580\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4501 - acc: 0.8395 - val_loss: 0.6427 - val_acc: 0.7590\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4470 - acc: 0.8401 - val_loss: 0.6417 - val_acc: 0.7620\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4443 - acc: 0.8421 - val_loss: 0.6408 - val_acc: 0.7570\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4412 - acc: 0.8432 - val_loss: 0.6418 - val_acc: 0.7570\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4389 - acc: 0.8427 - val_loss: 0.6420 - val_acc: 0.7580\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4355 - acc: 0.8436 - val_loss: 0.6420 - val_acc: 0.7580\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4331 - acc: 0.8461 - val_loss: 0.6414 - val_acc: 0.7650\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4303 - acc: 0.8480 - val_loss: 0.6400 - val_acc: 0.7640\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4282 - acc: 0.8472 - val_loss: 0.6415 - val_acc: 0.7620\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4252 - acc: 0.8484 - val_loss: 0.6467 - val_acc: 0.7570\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4227 - acc: 0.8497 - val_loss: 0.6417 - val_acc: 0.7640\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4196 - acc: 0.8521 - val_loss: 0.6417 - val_acc: 0.7630\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4173 - acc: 0.8523 - val_loss: 0.6426 - val_acc: 0.7610\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4145 - acc: 0.8535 - val_loss: 0.6399 - val_acc: 0.7630\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4118 - acc: 0.8547 - val_loss: 0.6418 - val_acc: 0.7640\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4094 - acc: 0.8544 - val_loss: 0.6429 - val_acc: 0.7650\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4067 - acc: 0.8553 - val_loss: 0.6414 - val_acc: 0.7640\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.4042 - acc: 0.8580 - val_loss: 0.6442 - val_acc: 0.7630\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4024 - acc: 0.8571 - val_loss: 0.6408 - val_acc: 0.7710\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3996 - acc: 0.8603 - val_loss: 0.6415 - val_acc: 0.7680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3970 - acc: 0.8600 - val_loss: 0.6416 - val_acc: 0.7690\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = mod.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = mod.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 19us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = mod.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.39394298666318256, 0.8632000000317891]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6532384389241537, 0.7580000001589458]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.9409 - acc: 0.1785 - val_loss: 1.9270 - val_acc: 0.2040\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.9164 - acc: 0.2099 - val_loss: 1.9051 - val_acc: 0.2260\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8939 - acc: 0.2331 - val_loss: 1.8826 - val_acc: 0.2530\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8697 - acc: 0.2563 - val_loss: 1.8592 - val_acc: 0.2750\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8416 - acc: 0.2836 - val_loss: 1.8317 - val_acc: 0.2950\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8080 - acc: 0.3139 - val_loss: 1.7978 - val_acc: 0.3270\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7676 - acc: 0.3565 - val_loss: 1.7568 - val_acc: 0.3680\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7210 - acc: 0.3965 - val_loss: 1.7116 - val_acc: 0.3910\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6691 - acc: 0.4320 - val_loss: 1.6627 - val_acc: 0.4230\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6139 - acc: 0.4557 - val_loss: 1.6086 - val_acc: 0.4520\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5561 - acc: 0.4849 - val_loss: 1.5553 - val_acc: 0.4640\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4985 - acc: 0.5045 - val_loss: 1.5028 - val_acc: 0.4850\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4418 - acc: 0.5280 - val_loss: 1.4494 - val_acc: 0.5050\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3870 - acc: 0.5469 - val_loss: 1.3993 - val_acc: 0.5180\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3340 - acc: 0.5660 - val_loss: 1.3507 - val_acc: 0.5550\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2838 - acc: 0.5876 - val_loss: 1.3039 - val_acc: 0.5780\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2359 - acc: 0.6051 - val_loss: 1.2590 - val_acc: 0.6040\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1912 - acc: 0.6223 - val_loss: 1.2184 - val_acc: 0.6160\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1488 - acc: 0.6405 - val_loss: 1.1816 - val_acc: 0.6300\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1094 - acc: 0.6533 - val_loss: 1.1439 - val_acc: 0.6340\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0719 - acc: 0.6655 - val_loss: 1.1114 - val_acc: 0.6540\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0376 - acc: 0.6743 - val_loss: 1.0786 - val_acc: 0.6740\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0049 - acc: 0.6861 - val_loss: 1.0492 - val_acc: 0.6740\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9746 - acc: 0.6933 - val_loss: 1.0232 - val_acc: 0.6850\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9461 - acc: 0.7032 - val_loss: 0.9956 - val_acc: 0.6890\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9196 - acc: 0.7087 - val_loss: 0.9724 - val_acc: 0.6970\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9010 - acc: 0.717 - 0s 23us/step - loss: 0.8950 - acc: 0.7161 - val_loss: 0.9513 - val_acc: 0.7010\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8716 - acc: 0.7201 - val_loss: 0.9317 - val_acc: 0.7070\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8501 - acc: 0.7227 - val_loss: 0.9120 - val_acc: 0.7070\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8302 - acc: 0.7277 - val_loss: 0.8952 - val_acc: 0.7150\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8113 - acc: 0.7357 - val_loss: 0.8809 - val_acc: 0.7060\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7941 - acc: 0.7356 - val_loss: 0.8650 - val_acc: 0.7150\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7779 - acc: 0.7409 - val_loss: 0.8510 - val_acc: 0.7280\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7626 - acc: 0.7467 - val_loss: 0.8368 - val_acc: 0.7290\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7479 - acc: 0.7476 - val_loss: 0.8273 - val_acc: 0.7260\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7350 - acc: 0.7511 - val_loss: 0.8145 - val_acc: 0.7370\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7222 - acc: 0.7535 - val_loss: 0.8048 - val_acc: 0.7340\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7100 - acc: 0.7569 - val_loss: 0.7951 - val_acc: 0.7370\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6991 - acc: 0.7597 - val_loss: 0.7881 - val_acc: 0.7360\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6884 - acc: 0.7629 - val_loss: 0.7778 - val_acc: 0.7400\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6784 - acc: 0.7637 - val_loss: 0.7710 - val_acc: 0.7390\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6688 - acc: 0.7653 - val_loss: 0.7638 - val_acc: 0.7430\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6597 - acc: 0.7715 - val_loss: 0.7591 - val_acc: 0.7350\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6513 - acc: 0.7696 - val_loss: 0.7521 - val_acc: 0.7380\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6429 - acc: 0.7744 - val_loss: 0.7471 - val_acc: 0.7410\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.6350 - acc: 0.7780 - val_loss: 0.7390 - val_acc: 0.7450\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6278 - acc: 0.7785 - val_loss: 0.7349 - val_acc: 0.7500\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6204 - acc: 0.7797 - val_loss: 0.7307 - val_acc: 0.7530\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6137 - acc: 0.7816 - val_loss: 0.7233 - val_acc: 0.7460\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6069 - acc: 0.7840 - val_loss: 0.7188 - val_acc: 0.7490\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.6008 - acc: 0.785 - 0s 24us/step - loss: 0.6006 - acc: 0.7851 - val_loss: 0.7153 - val_acc: 0.7500\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5948 - acc: 0.7872 - val_loss: 0.7129 - val_acc: 0.7440\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5883 - acc: 0.7889 - val_loss: 0.7087 - val_acc: 0.7460\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5828 - acc: 0.7909 - val_loss: 0.7047 - val_acc: 0.7460\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5770 - acc: 0.7932 - val_loss: 0.7011 - val_acc: 0.7490\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5718 - acc: 0.7948 - val_loss: 0.6982 - val_acc: 0.7490\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5664 - acc: 0.7956 - val_loss: 0.6953 - val_acc: 0.7530\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5613 - acc: 0.7971 - val_loss: 0.6937 - val_acc: 0.7520\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5567 - acc: 0.7980 - val_loss: 0.6895 - val_acc: 0.7460\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.5517 - acc: 0.8009 - val_loss: 0.6872 - val_acc: 0.7520\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 19us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5469161385536194, 0.8028]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6952074344952901, 0.7433333338101705]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.6114 - acc: 0.1895 - val_loss: 2.5684 - val_acc: 0.2100\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.5639 - acc: 0.2116 - val_loss: 2.5367 - val_acc: 0.2200\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.5320 - acc: 0.2179 - val_loss: 2.5113 - val_acc: 0.2300\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.5033 - acc: 0.2313 - val_loss: 2.4843 - val_acc: 0.2480\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.4730 - acc: 0.2507 - val_loss: 2.4548 - val_acc: 0.2690\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.4399 - acc: 0.2760 - val_loss: 2.4193 - val_acc: 0.3030\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.4028 - acc: 0.3141 - val_loss: 2.3807 - val_acc: 0.3220\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.3627 - acc: 0.3409 - val_loss: 2.3386 - val_acc: 0.3680\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.3201 - acc: 0.3733 - val_loss: 2.2946 - val_acc: 0.3910\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.2756 - acc: 0.3997 - val_loss: 2.2501 - val_acc: 0.4120\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.2298 - acc: 0.4213 - val_loss: 2.2051 - val_acc: 0.4440\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.1836 - acc: 0.4483 - val_loss: 2.1593 - val_acc: 0.4650\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.1359 - acc: 0.4728 - val_loss: 2.1133 - val_acc: 0.4960\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.0871 - acc: 0.4989 - val_loss: 2.0664 - val_acc: 0.5240\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.0377 - acc: 0.5284 - val_loss: 2.0190 - val_acc: 0.5460\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9883 - acc: 0.5541 - val_loss: 1.9733 - val_acc: 0.5680\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9397 - acc: 0.5783 - val_loss: 1.9287 - val_acc: 0.5890\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8918 - acc: 0.6020 - val_loss: 1.8836 - val_acc: 0.6030\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8452 - acc: 0.6227 - val_loss: 1.8403 - val_acc: 0.6120\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8009 - acc: 0.6368 - val_loss: 1.8014 - val_acc: 0.6230\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7584 - acc: 0.6531 - val_loss: 1.7615 - val_acc: 0.6370\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7183 - acc: 0.6659 - val_loss: 1.7250 - val_acc: 0.6490\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6803 - acc: 0.6764 - val_loss: 1.6910 - val_acc: 0.6590\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6448 - acc: 0.6855 - val_loss: 1.6595 - val_acc: 0.6680\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6118 - acc: 0.6936 - val_loss: 1.6291 - val_acc: 0.6770\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5805 - acc: 0.7024 - val_loss: 1.6005 - val_acc: 0.6840\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5515 - acc: 0.7076 - val_loss: 1.5749 - val_acc: 0.6900\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5244 - acc: 0.7140 - val_loss: 1.5499 - val_acc: 0.7010\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4988 - acc: 0.7211 - val_loss: 1.5288 - val_acc: 0.7070\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4747 - acc: 0.7245 - val_loss: 1.5082 - val_acc: 0.7050\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4520 - acc: 0.7307 - val_loss: 1.4893 - val_acc: 0.7170\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4314 - acc: 0.7372 - val_loss: 1.4701 - val_acc: 0.7220\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4116 - acc: 0.7409 - val_loss: 1.4548 - val_acc: 0.7280\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3928 - acc: 0.7465 - val_loss: 1.4380 - val_acc: 0.7230\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3752 - acc: 0.7496 - val_loss: 1.4220 - val_acc: 0.7230\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3584 - acc: 0.7563 - val_loss: 1.4078 - val_acc: 0.7340\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3425 - acc: 0.7627 - val_loss: 1.3935 - val_acc: 0.7380\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3276 - acc: 0.7651 - val_loss: 1.3827 - val_acc: 0.7420\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3136 - acc: 0.7655 - val_loss: 1.3730 - val_acc: 0.7390\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3003 - acc: 0.7695 - val_loss: 1.3623 - val_acc: 0.7360\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2876 - acc: 0.7741 - val_loss: 1.3491 - val_acc: 0.7370\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2747 - acc: 0.7764 - val_loss: 1.3409 - val_acc: 0.7330\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2631 - acc: 0.7776 - val_loss: 1.3330 - val_acc: 0.7350\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2517 - acc: 0.7803 - val_loss: 1.3214 - val_acc: 0.7510\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2405 - acc: 0.7848 - val_loss: 1.3135 - val_acc: 0.7510\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2301 - acc: 0.7864 - val_loss: 1.3052 - val_acc: 0.7560\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2203 - acc: 0.7880 - val_loss: 1.2986 - val_acc: 0.7330\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2102 - acc: 0.7909 - val_loss: 1.2917 - val_acc: 0.7480\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2008 - acc: 0.7937 - val_loss: 1.2836 - val_acc: 0.7550\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1920 - acc: 0.7971 - val_loss: 1.2770 - val_acc: 0.7430\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1834 - acc: 0.7963 - val_loss: 1.2706 - val_acc: 0.7610\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1746 - acc: 0.8003 - val_loss: 1.2630 - val_acc: 0.7590\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1660 - acc: 0.8001 - val_loss: 1.2579 - val_acc: 0.7600\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1580 - acc: 0.8033 - val_loss: 1.2505 - val_acc: 0.7660\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1504 - acc: 0.8031 - val_loss: 1.2452 - val_acc: 0.7650\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1426 - acc: 0.8073 - val_loss: 1.2403 - val_acc: 0.7640\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1351 - acc: 0.8085 - val_loss: 1.2355 - val_acc: 0.7510\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1280 - acc: 0.8093 - val_loss: 1.2297 - val_acc: 0.7590\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1211 - acc: 0.8095 - val_loss: 1.2251 - val_acc: 0.7630\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1139 - acc: 0.8115 - val_loss: 1.2218 - val_acc: 0.7570\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1069 - acc: 0.8155 - val_loss: 1.2172 - val_acc: 0.7650\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1006 - acc: 0.8145 - val_loss: 1.2103 - val_acc: 0.7620\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0941 - acc: 0.8164 - val_loss: 1.2063 - val_acc: 0.7570\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0876 - acc: 0.8197 - val_loss: 1.2016 - val_acc: 0.7670\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0810 - acc: 0.8183 - val_loss: 1.1986 - val_acc: 0.7650\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0753 - acc: 0.8223 - val_loss: 1.1939 - val_acc: 0.7730\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0689 - acc: 0.8211 - val_loss: 1.1896 - val_acc: 0.7670\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0633 - acc: 0.8232 - val_loss: 1.1890 - val_acc: 0.7680\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0571 - acc: 0.8240 - val_loss: 1.1837 - val_acc: 0.7640\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0518 - acc: 0.8277 - val_loss: 1.1770 - val_acc: 0.7700\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0459 - acc: 0.8275 - val_loss: 1.1756 - val_acc: 0.7710\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0403 - acc: 0.8288 - val_loss: 1.1699 - val_acc: 0.7760\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0348 - acc: 0.8305 - val_loss: 1.1689 - val_acc: 0.7680\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0293 - acc: 0.8329 - val_loss: 1.1654 - val_acc: 0.7740\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0241 - acc: 0.8353 - val_loss: 1.1599 - val_acc: 0.7740\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0187 - acc: 0.8335 - val_loss: 1.1579 - val_acc: 0.7740\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0135 - acc: 0.8361 - val_loss: 1.1560 - val_acc: 0.7720\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0090 - acc: 0.8377 - val_loss: 1.1543 - val_acc: 0.7730\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0033 - acc: 0.8388 - val_loss: 1.1491 - val_acc: 0.7740\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9986 - acc: 0.8399 - val_loss: 1.1448 - val_acc: 0.7720\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9937 - acc: 0.8417 - val_loss: 1.1428 - val_acc: 0.7740\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9887 - acc: 0.8437 - val_loss: 1.1414 - val_acc: 0.7790\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9838 - acc: 0.8455 - val_loss: 1.1396 - val_acc: 0.7800\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9791 - acc: 0.8455 - val_loss: 1.1366 - val_acc: 0.7850\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9747 - acc: 0.8453 - val_loss: 1.1333 - val_acc: 0.7750\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9689 - acc: 0.848 - 0s 25us/step - loss: 0.9697 - acc: 0.8488 - val_loss: 1.1298 - val_acc: 0.7830\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9647 - acc: 0.8471 - val_loss: 1.1257 - val_acc: 0.7740\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9603 - acc: 0.8515 - val_loss: 1.1240 - val_acc: 0.7810\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9560 - acc: 0.8504 - val_loss: 1.1194 - val_acc: 0.7820\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9517 - acc: 0.8511 - val_loss: 1.1182 - val_acc: 0.7790\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9471 - acc: 0.8528 - val_loss: 1.1161 - val_acc: 0.7780\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9426 - acc: 0.8548 - val_loss: 1.1134 - val_acc: 0.7800\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9384 - acc: 0.8545 - val_loss: 1.1114 - val_acc: 0.7810\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9347 - acc: 0.8567 - val_loss: 1.1079 - val_acc: 0.7820\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9299 - acc: 0.8584 - val_loss: 1.1067 - val_acc: 0.7790\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9260 - acc: 0.8589 - val_loss: 1.1059 - val_acc: 0.7820\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9219 - acc: 0.8591 - val_loss: 1.1024 - val_acc: 0.7800\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9178 - acc: 0.8601 - val_loss: 1.1028 - val_acc: 0.7840\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9141 - acc: 0.8609 - val_loss: 1.0971 - val_acc: 0.7790\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9096 - acc: 0.8623 - val_loss: 1.0953 - val_acc: 0.7760\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9054 - acc: 0.8648 - val_loss: 1.0955 - val_acc: 0.7830\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9020 - acc: 0.8647 - val_loss: 1.0960 - val_acc: 0.7810\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8980 - acc: 0.8660 - val_loss: 1.0886 - val_acc: 0.7730\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8942 - acc: 0.8660 - val_loss: 1.0880 - val_acc: 0.7750\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8905 - acc: 0.8667 - val_loss: 1.0847 - val_acc: 0.7790\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8867 - acc: 0.8687 - val_loss: 1.0821 - val_acc: 0.7810\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8826 - acc: 0.8680 - val_loss: 1.0812 - val_acc: 0.7810\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8791 - acc: 0.8684 - val_loss: 1.0806 - val_acc: 0.7740\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8751 - acc: 0.8696 - val_loss: 1.0763 - val_acc: 0.7750\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8718 - acc: 0.8684 - val_loss: 1.0761 - val_acc: 0.7840\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8681 - acc: 0.8716 - val_loss: 1.0748 - val_acc: 0.7820\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8645 - acc: 0.8724 - val_loss: 1.0721 - val_acc: 0.7850\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8610 - acc: 0.8709 - val_loss: 1.0694 - val_acc: 0.7760\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8575 - acc: 0.8739 - val_loss: 1.0686 - val_acc: 0.7720\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8539 - acc: 0.8736 - val_loss: 1.0676 - val_acc: 0.7690\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8504 - acc: 0.8761 - val_loss: 1.0655 - val_acc: 0.7830\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8469 - acc: 0.8772 - val_loss: 1.0660 - val_acc: 0.7720\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8435 - acc: 0.8772 - val_loss: 1.0658 - val_acc: 0.7650\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8404 - acc: 0.8769 - val_loss: 1.0608 - val_acc: 0.7850\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8369 - acc: 0.8783 - val_loss: 1.0573 - val_acc: 0.7740\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 15.9631 - acc: 0.1669 - val_loss: 15.5563 - val_acc: 0.1840\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 15.1998 - acc: 0.2165 - val_loss: 14.8069 - val_acc: 0.2180\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 14.4598 - acc: 0.2492 - val_loss: 14.0794 - val_acc: 0.2480\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 13.7404 - acc: 0.2712 - val_loss: 13.3717 - val_acc: 0.2700\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 13.0394 - acc: 0.2987 - val_loss: 12.6819 - val_acc: 0.2950\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 12.3558 - acc: 0.3315 - val_loss: 12.0095 - val_acc: 0.3340\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 11.6896 - acc: 0.3648 - val_loss: 11.3548 - val_acc: 0.3690\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 11.0416 - acc: 0.3935 - val_loss: 10.7174 - val_acc: 0.4060\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 10.4117 - acc: 0.4276 - val_loss: 10.0975 - val_acc: 0.4250\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 9.8011 - acc: 0.4580 - val_loss: 9.4991 - val_acc: 0.4760\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 9.2112 - acc: 0.4929 - val_loss: 8.9215 - val_acc: 0.5200\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 8.6443 - acc: 0.5257 - val_loss: 8.3694 - val_acc: 0.5380\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 8.1020 - acc: 0.5463 - val_loss: 7.8401 - val_acc: 0.5670\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 7.5848 - acc: 0.5639 - val_loss: 7.3362 - val_acc: 0.5800\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 7.0918 - acc: 0.5845 - val_loss: 6.8592 - val_acc: 0.6170\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 6.6239 - acc: 0.6063 - val_loss: 6.4014 - val_acc: 0.6170\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 6.1799 - acc: 0.6205 - val_loss: 5.9699 - val_acc: 0.6140\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 5.7610 - acc: 0.6355 - val_loss: 5.5651 - val_acc: 0.6330\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 5.3666 - acc: 0.6452 - val_loss: 5.1815 - val_acc: 0.6420\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 4.9961 - acc: 0.6533 - val_loss: 4.8238 - val_acc: 0.6600\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 4.6483 - acc: 0.6641 - val_loss: 4.4895 - val_acc: 0.6590\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 4.3244 - acc: 0.6744 - val_loss: 4.1768 - val_acc: 0.6690\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 4.0232 - acc: 0.6776 - val_loss: 3.8885 - val_acc: 0.6730\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 3.7449 - acc: 0.6829 - val_loss: 3.6190 - val_acc: 0.6790\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 3.4886 - acc: 0.6868 - val_loss: 3.3740 - val_acc: 0.6730\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 3.2543 - acc: 0.6876 - val_loss: 3.1536 - val_acc: 0.6870\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 3.0420 - acc: 0.6924 - val_loss: 2.9490 - val_acc: 0.6870\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.8512 - acc: 0.6915 - val_loss: 2.7700 - val_acc: 0.6980\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.6809 - acc: 0.6959 - val_loss: 2.6113 - val_acc: 0.6970\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.5316 - acc: 0.6973 - val_loss: 2.4712 - val_acc: 0.6930\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.4031 - acc: 0.6984 - val_loss: 2.3522 - val_acc: 0.7000\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.2942 - acc: 0.6987 - val_loss: 2.2552 - val_acc: 0.6980\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.2049 - acc: 0.6992 - val_loss: 2.1762 - val_acc: 0.6960\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.1333 - acc: 0.7000 - val_loss: 2.1097 - val_acc: 0.6910\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.0774 - acc: 0.6975 - val_loss: 2.0605 - val_acc: 0.6880\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.0355 - acc: 0.6987 - val_loss: 2.0251 - val_acc: 0.6890\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.0037 - acc: 0.6979 - val_loss: 1.9984 - val_acc: 0.6940\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.9788 - acc: 0.6985 - val_loss: 1.9760 - val_acc: 0.6900\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9573 - acc: 0.6955 - val_loss: 1.9537 - val_acc: 0.6880\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9381 - acc: 0.7001 - val_loss: 1.9359 - val_acc: 0.6900\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9203 - acc: 0.6997 - val_loss: 1.9217 - val_acc: 0.6880\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9042 - acc: 0.7007 - val_loss: 1.9084 - val_acc: 0.6910\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8884 - acc: 0.7015 - val_loss: 1.8949 - val_acc: 0.6900\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8737 - acc: 0.7032 - val_loss: 1.8777 - val_acc: 0.6920\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8593 - acc: 0.7029 - val_loss: 1.8655 - val_acc: 0.6920\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8463 - acc: 0.7013 - val_loss: 1.8526 - val_acc: 0.7000\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8330 - acc: 0.7029 - val_loss: 1.8452 - val_acc: 0.6990\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.8206 - acc: 0.7023 - val_loss: 1.8278 - val_acc: 0.6910\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8086 - acc: 0.7045 - val_loss: 1.8180 - val_acc: 0.7050\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7971 - acc: 0.7043 - val_loss: 1.8041 - val_acc: 0.7040\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7851 - acc: 0.7044 - val_loss: 1.7943 - val_acc: 0.6980\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7742 - acc: 0.7036 - val_loss: 1.7834 - val_acc: 0.6950\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7632 - acc: 0.7045 - val_loss: 1.7732 - val_acc: 0.7040\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7529 - acc: 0.7055 - val_loss: 1.7649 - val_acc: 0.7050\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7426 - acc: 0.7061 - val_loss: 1.7537 - val_acc: 0.7010\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7326 - acc: 0.7055 - val_loss: 1.7441 - val_acc: 0.6990\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7230 - acc: 0.7075 - val_loss: 1.7328 - val_acc: 0.7030\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7134 - acc: 0.7091 - val_loss: 1.7252 - val_acc: 0.6980\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7038 - acc: 0.7089 - val_loss: 1.7196 - val_acc: 0.7010\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6950 - acc: 0.7088 - val_loss: 1.7085 - val_acc: 0.6990\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6860 - acc: 0.7085 - val_loss: 1.6988 - val_acc: 0.7020\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6772 - acc: 0.7083 - val_loss: 1.6913 - val_acc: 0.7020\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6687 - acc: 0.7092 - val_loss: 1.6862 - val_acc: 0.7000\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6604 - acc: 0.7117 - val_loss: 1.6739 - val_acc: 0.7030\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6516 - acc: 0.7116 - val_loss: 1.6726 - val_acc: 0.6990\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6440 - acc: 0.7108 - val_loss: 1.6588 - val_acc: 0.7000\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6350 - acc: 0.7109 - val_loss: 1.6519 - val_acc: 0.7080\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6274 - acc: 0.7104 - val_loss: 1.6433 - val_acc: 0.7010\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6196 - acc: 0.7117 - val_loss: 1.6395 - val_acc: 0.7050\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6121 - acc: 0.7101 - val_loss: 1.6327 - val_acc: 0.7090\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6042 - acc: 0.7128 - val_loss: 1.6259 - val_acc: 0.7070\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5965 - acc: 0.7119 - val_loss: 1.6151 - val_acc: 0.7070\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5898 - acc: 0.7133 - val_loss: 1.6077 - val_acc: 0.7070\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5819 - acc: 0.7127 - val_loss: 1.6050 - val_acc: 0.7060\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5750 - acc: 0.7139 - val_loss: 1.5955 - val_acc: 0.7030\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5673 - acc: 0.7144 - val_loss: 1.5897 - val_acc: 0.7040\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5606 - acc: 0.7159 - val_loss: 1.5819 - val_acc: 0.7040\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5541 - acc: 0.7136 - val_loss: 1.5732 - val_acc: 0.7070\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5469 - acc: 0.7143 - val_loss: 1.5667 - val_acc: 0.7060\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5401 - acc: 0.7156 - val_loss: 1.5629 - val_acc: 0.7050\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5337 - acc: 0.7164 - val_loss: 1.5534 - val_acc: 0.7050\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5269 - acc: 0.7147 - val_loss: 1.5500 - val_acc: 0.7110\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5208 - acc: 0.7179 - val_loss: 1.5438 - val_acc: 0.7110\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5144 - acc: 0.7168 - val_loss: 1.5363 - val_acc: 0.7070\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5078 - acc: 0.7169 - val_loss: 1.5309 - val_acc: 0.7120\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5018 - acc: 0.7179 - val_loss: 1.5284 - val_acc: 0.7110\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4949 - acc: 0.7187 - val_loss: 1.5196 - val_acc: 0.7090\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4893 - acc: 0.7179 - val_loss: 1.5185 - val_acc: 0.7040\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4833 - acc: 0.7180 - val_loss: 1.5076 - val_acc: 0.7130\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4770 - acc: 0.7193 - val_loss: 1.5028 - val_acc: 0.7120\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4710 - acc: 0.7187 - val_loss: 1.4976 - val_acc: 0.7080\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4648 - acc: 0.7188 - val_loss: 1.4895 - val_acc: 0.7120\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4590 - acc: 0.7201 - val_loss: 1.4853 - val_acc: 0.7140\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4529 - acc: 0.7201 - val_loss: 1.4790 - val_acc: 0.7120\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4473 - acc: 0.7204 - val_loss: 1.4739 - val_acc: 0.7110\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4419 - acc: 0.7209 - val_loss: 1.4687 - val_acc: 0.7090\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4350 - acc: 0.7201 - val_loss: 1.4636 - val_acc: 0.7090\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4299 - acc: 0.7197 - val_loss: 1.4589 - val_acc: 0.7130\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4242 - acc: 0.7223 - val_loss: 1.4517 - val_acc: 0.7130\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4188 - acc: 0.7212 - val_loss: 1.4464 - val_acc: 0.7100\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4139 - acc: 0.7227 - val_loss: 1.4408 - val_acc: 0.7130\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4079 - acc: 0.7227 - val_loss: 1.4368 - val_acc: 0.7100\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4028 - acc: 0.7235 - val_loss: 1.4326 - val_acc: 0.7130\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3978 - acc: 0.7223 - val_loss: 1.4327 - val_acc: 0.7130\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3922 - acc: 0.7224 - val_loss: 1.4218 - val_acc: 0.7120\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3870 - acc: 0.7217 - val_loss: 1.4195 - val_acc: 0.7120\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3824 - acc: 0.7241 - val_loss: 1.4107 - val_acc: 0.7100\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3769 - acc: 0.7239 - val_loss: 1.4067 - val_acc: 0.7120\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3718 - acc: 0.7236 - val_loss: 1.4029 - val_acc: 0.7120\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3668 - acc: 0.7251 - val_loss: 1.3969 - val_acc: 0.7160\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3622 - acc: 0.7251 - val_loss: 1.3942 - val_acc: 0.7120\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3569 - acc: 0.7247 - val_loss: 1.3889 - val_acc: 0.7140\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3527 - acc: 0.7245 - val_loss: 1.3820 - val_acc: 0.7150\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3478 - acc: 0.7259 - val_loss: 1.3802 - val_acc: 0.7170\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3431 - acc: 0.7257 - val_loss: 1.3757 - val_acc: 0.7170\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3383 - acc: 0.7249 - val_loss: 1.3738 - val_acc: 0.7110\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3335 - acc: 0.7260 - val_loss: 1.3708 - val_acc: 0.7150\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3293 - acc: 0.7264 - val_loss: 1.3640 - val_acc: 0.7180\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3252 - acc: 0.7247 - val_loss: 1.3549 - val_acc: 0.7190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3202 - acc: 0.7263 - val_loss: 1.3567 - val_acc: 0.7150\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 16.0295 - acc: 0.1207 - val_loss: 15.6155 - val_acc: 0.1320\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 15.2659 - acc: 0.1377 - val_loss: 14.8704 - val_acc: 0.1600\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 14.5344 - acc: 0.1700 - val_loss: 14.1524 - val_acc: 0.1880\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 13.8273 - acc: 0.2021 - val_loss: 13.4582 - val_acc: 0.2150\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 13.1429 - acc: 0.2328 - val_loss: 12.7858 - val_acc: 0.2460\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 12.4798 - acc: 0.2543 - val_loss: 12.1338 - val_acc: 0.2650\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 11.8367 - acc: 0.2715 - val_loss: 11.5016 - val_acc: 0.2750\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 11.2133 - acc: 0.2861 - val_loss: 10.8894 - val_acc: 0.3030\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 10.6100 - acc: 0.3064 - val_loss: 10.2970 - val_acc: 0.3190\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 10.0265 - acc: 0.3296 - val_loss: 9.7247 - val_acc: 0.3450\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 9.4631 - acc: 0.3491 - val_loss: 9.1728 - val_acc: 0.3700\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 8.9197 - acc: 0.3720 - val_loss: 8.6422 - val_acc: 0.3910\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 8.3966 - acc: 0.3961 - val_loss: 8.1303 - val_acc: 0.4060\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 7.8938 - acc: 0.4149 - val_loss: 7.6399 - val_acc: 0.4260\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 7.4116 - acc: 0.4311 - val_loss: 7.1695 - val_acc: 0.4430\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 6.9492 - acc: 0.4507 - val_loss: 6.7188 - val_acc: 0.4680\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 6.5063 - acc: 0.4664 - val_loss: 6.2883 - val_acc: 0.4770\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 6.0843 - acc: 0.4807 - val_loss: 5.8787 - val_acc: 0.4850\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 5.6844 - acc: 0.4921 - val_loss: 5.4913 - val_acc: 0.5060\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 5.3060 - acc: 0.5100 - val_loss: 5.1252 - val_acc: 0.5250\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 4.9499 - acc: 0.5237 - val_loss: 4.7819 - val_acc: 0.5400\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 4.6162 - acc: 0.5372 - val_loss: 4.4599 - val_acc: 0.5420\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 4.3048 - acc: 0.5412 - val_loss: 4.1593 - val_acc: 0.5500\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 4.0145 - acc: 0.5525 - val_loss: 3.8814 - val_acc: 0.5550\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 3.7469 - acc: 0.5576 - val_loss: 3.6254 - val_acc: 0.5670\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 3.5010 - acc: 0.5695 - val_loss: 3.3907 - val_acc: 0.5600\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 3.2771 - acc: 0.5788 - val_loss: 3.1774 - val_acc: 0.5900\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 3.0752 - acc: 0.5891 - val_loss: 2.9879 - val_acc: 0.5930\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.8959 - acc: 0.5996 - val_loss: 2.8178 - val_acc: 0.5940\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.7376 - acc: 0.6017 - val_loss: 2.6726 - val_acc: 0.5900\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.5997 - acc: 0.6096 - val_loss: 2.5447 - val_acc: 0.6030\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.4819 - acc: 0.6155 - val_loss: 2.4358 - val_acc: 0.6090\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.3833 - acc: 0.6215 - val_loss: 2.3480 - val_acc: 0.6110\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.3035 - acc: 0.6268 - val_loss: 2.2768 - val_acc: 0.6090\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.2408 - acc: 0.6271 - val_loss: 2.2245 - val_acc: 0.6170\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.1932 - acc: 0.6341 - val_loss: 2.1829 - val_acc: 0.6290\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.1566 - acc: 0.6355 - val_loss: 2.1478 - val_acc: 0.6240\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.1263 - acc: 0.6340 - val_loss: 2.1202 - val_acc: 0.6240\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.0992 - acc: 0.6383 - val_loss: 2.0946 - val_acc: 0.6220\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.0745 - acc: 0.6400 - val_loss: 2.0749 - val_acc: 0.6330\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.0522 - acc: 0.6475 - val_loss: 2.0500 - val_acc: 0.6250\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.0304 - acc: 0.6471 - val_loss: 2.0296 - val_acc: 0.6300\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.0100 - acc: 0.6504 - val_loss: 2.0123 - val_acc: 0.6320\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9908 - acc: 0.6541 - val_loss: 1.9906 - val_acc: 0.6370\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9724 - acc: 0.6543 - val_loss: 1.9743 - val_acc: 0.6360\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9543 - acc: 0.6569 - val_loss: 1.9566 - val_acc: 0.6410\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9375 - acc: 0.6573 - val_loss: 1.9415 - val_acc: 0.6430\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.9218 - acc: 0.6625 - val_loss: 1.9282 - val_acc: 0.6420\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9061 - acc: 0.6629 - val_loss: 1.9105 - val_acc: 0.6420\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8911 - acc: 0.6636 - val_loss: 1.8962 - val_acc: 0.6460\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8767 - acc: 0.6652 - val_loss: 1.8858 - val_acc: 0.6510\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8627 - acc: 0.6677 - val_loss: 1.8707 - val_acc: 0.6530\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8497 - acc: 0.6680 - val_loss: 1.8620 - val_acc: 0.6560\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8369 - acc: 0.6720 - val_loss: 1.8452 - val_acc: 0.6530\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8240 - acc: 0.6697 - val_loss: 1.8336 - val_acc: 0.6570\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8121 - acc: 0.6725 - val_loss: 1.8226 - val_acc: 0.6570\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8007 - acc: 0.6739 - val_loss: 1.8124 - val_acc: 0.6610\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7890 - acc: 0.6748 - val_loss: 1.8013 - val_acc: 0.6560\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7784 - acc: 0.6773 - val_loss: 1.7923 - val_acc: 0.6580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7679 - acc: 0.6761 - val_loss: 1.7803 - val_acc: 0.6600\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7571 - acc: 0.6765 - val_loss: 1.7711 - val_acc: 0.6590\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7468 - acc: 0.6796 - val_loss: 1.7628 - val_acc: 0.6620\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7370 - acc: 0.6797 - val_loss: 1.7522 - val_acc: 0.6640\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7270 - acc: 0.6801 - val_loss: 1.7419 - val_acc: 0.6630\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7174 - acc: 0.6817 - val_loss: 1.7321 - val_acc: 0.6620\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7082 - acc: 0.6823 - val_loss: 1.7274 - val_acc: 0.6660\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6996 - acc: 0.6831 - val_loss: 1.7147 - val_acc: 0.6670\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6904 - acc: 0.6839 - val_loss: 1.7089 - val_acc: 0.6590\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6821 - acc: 0.6828 - val_loss: 1.7009 - val_acc: 0.6730\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6730 - acc: 0.6853 - val_loss: 1.6933 - val_acc: 0.6780\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6645 - acc: 0.6853 - val_loss: 1.6805 - val_acc: 0.6750\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6558 - acc: 0.6852 - val_loss: 1.6738 - val_acc: 0.6800\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6474 - acc: 0.6863 - val_loss: 1.6661 - val_acc: 0.6780\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6393 - acc: 0.6876 - val_loss: 1.6591 - val_acc: 0.6790\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6313 - acc: 0.6884 - val_loss: 1.6503 - val_acc: 0.6800\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6231 - acc: 0.6880 - val_loss: 1.6417 - val_acc: 0.6810\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6155 - acc: 0.6888 - val_loss: 1.6385 - val_acc: 0.6800\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6079 - acc: 0.6892 - val_loss: 1.6272 - val_acc: 0.6800\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6000 - acc: 0.6904 - val_loss: 1.6229 - val_acc: 0.6840\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5926 - acc: 0.6907 - val_loss: 1.6140 - val_acc: 0.6830\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5846 - acc: 0.6913 - val_loss: 1.6049 - val_acc: 0.6840\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5774 - acc: 0.6904 - val_loss: 1.5993 - val_acc: 0.6830\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5702 - acc: 0.6923 - val_loss: 1.5904 - val_acc: 0.6840\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5629 - acc: 0.6927 - val_loss: 1.5848 - val_acc: 0.6850\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5561 - acc: 0.6917 - val_loss: 1.5777 - val_acc: 0.6850\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5490 - acc: 0.6935 - val_loss: 1.5725 - val_acc: 0.6850\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5418 - acc: 0.6929 - val_loss: 1.5663 - val_acc: 0.6860\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5344 - acc: 0.6968 - val_loss: 1.5578 - val_acc: 0.6860\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5280 - acc: 0.6939 - val_loss: 1.5503 - val_acc: 0.6870\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5207 - acc: 0.6952 - val_loss: 1.5436 - val_acc: 0.6900\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5147 - acc: 0.6961 - val_loss: 1.5392 - val_acc: 0.6910\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5079 - acc: 0.6965 - val_loss: 1.5356 - val_acc: 0.6920\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5013 - acc: 0.6976 - val_loss: 1.5267 - val_acc: 0.6930\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4951 - acc: 0.6988 - val_loss: 1.5186 - val_acc: 0.6870\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4879 - acc: 0.6987 - val_loss: 1.5122 - val_acc: 0.6940\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4820 - acc: 0.7009 - val_loss: 1.5108 - val_acc: 0.6920\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4761 - acc: 0.6999 - val_loss: 1.4997 - val_acc: 0.6940\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4696 - acc: 0.7011 - val_loss: 1.4934 - val_acc: 0.6900\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4639 - acc: 0.7003 - val_loss: 1.4879 - val_acc: 0.6960\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4571 - acc: 0.7012 - val_loss: 1.4841 - val_acc: 0.6970\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4517 - acc: 0.7027 - val_loss: 1.4777 - val_acc: 0.6970\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4459 - acc: 0.7016 - val_loss: 1.4723 - val_acc: 0.6930\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4400 - acc: 0.7021 - val_loss: 1.4649 - val_acc: 0.6960\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4344 - acc: 0.7029 - val_loss: 1.4617 - val_acc: 0.6900\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4283 - acc: 0.7043 - val_loss: 1.4545 - val_acc: 0.6970\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4229 - acc: 0.7037 - val_loss: 1.4482 - val_acc: 0.6980\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4169 - acc: 0.7040 - val_loss: 1.4452 - val_acc: 0.6930\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4115 - acc: 0.7031 - val_loss: 1.4380 - val_acc: 0.6960\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4061 - acc: 0.7063 - val_loss: 1.4359 - val_acc: 0.6960\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4009 - acc: 0.7064 - val_loss: 1.4271 - val_acc: 0.6960\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3954 - acc: 0.7059 - val_loss: 1.4241 - val_acc: 0.6950\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3906 - acc: 0.7068 - val_loss: 1.4188 - val_acc: 0.6980\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3844 - acc: 0.7055 - val_loss: 1.4153 - val_acc: 0.6960\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3795 - acc: 0.7072 - val_loss: 1.4077 - val_acc: 0.6950\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3742 - acc: 0.7081 - val_loss: 1.4033 - val_acc: 0.7000\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3689 - acc: 0.7083 - val_loss: 1.3962 - val_acc: 0.7010\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3638 - acc: 0.7089 - val_loss: 1.3924 - val_acc: 0.6980\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3586 - acc: 0.7095 - val_loss: 1.3895 - val_acc: 0.7020\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3538 - acc: 0.7099 - val_loss: 1.3839 - val_acc: 0.6910\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3493 - acc: 0.7097 - val_loss: 1.3798 - val_acc: 0.6920\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3444 - acc: 0.7093 - val_loss: 1.3743 - val_acc: 0.7010\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3396 - acc: 0.7103 - val_loss: 1.3708 - val_acc: 0.6980\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3350 - acc: 0.7111 - val_loss: 1.3688 - val_acc: 0.6970\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3303 - acc: 0.7120 - val_loss: 1.3607 - val_acc: 0.7020\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3258 - acc: 0.7123 - val_loss: 1.3558 - val_acc: 0.7010\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3217 - acc: 0.7116 - val_loss: 1.3520 - val_acc: 0.7030\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3160 - acc: 0.7139 - val_loss: 1.3544 - val_acc: 0.6920\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3119 - acc: 0.7137 - val_loss: 1.3414 - val_acc: 0.7050\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3077 - acc: 0.7133 - val_loss: 1.3415 - val_acc: 0.7050\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3034 - acc: 0.7145 - val_loss: 1.3353 - val_acc: 0.7030\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2991 - acc: 0.7147 - val_loss: 1.3311 - val_acc: 0.7060\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2948 - acc: 0.7153 - val_loss: 1.3274 - val_acc: 0.6980\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2909 - acc: 0.7145 - val_loss: 1.3237 - val_acc: 0.7010\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2864 - acc: 0.7149 - val_loss: 1.3184 - val_acc: 0.7060\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2820 - acc: 0.7167 - val_loss: 1.3153 - val_acc: 0.7030\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2784 - acc: 0.7159 - val_loss: 1.3097 - val_acc: 0.7000\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2745 - acc: 0.7167 - val_loss: 1.3071 - val_acc: 0.7010\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2698 - acc: 0.7177 - val_loss: 1.3035 - val_acc: 0.7090\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2665 - acc: 0.7163 - val_loss: 1.3023 - val_acc: 0.7030\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2634 - acc: 0.7185 - val_loss: 1.2949 - val_acc: 0.7020\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2582 - acc: 0.7173 - val_loss: 1.2948 - val_acc: 0.7020\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2546 - acc: 0.7191 - val_loss: 1.2881 - val_acc: 0.7050\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2511 - acc: 0.7180 - val_loss: 1.2829 - val_acc: 0.7020\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2469 - acc: 0.7189 - val_loss: 1.2815 - val_acc: 0.7040\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2431 - acc: 0.7193 - val_loss: 1.2778 - val_acc: 0.7030\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2397 - acc: 0.7200 - val_loss: 1.2738 - val_acc: 0.7060\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2366 - acc: 0.7203 - val_loss: 1.2735 - val_acc: 0.7030\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2325 - acc: 0.7203 - val_loss: 1.2677 - val_acc: 0.7080\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2298 - acc: 0.7197 - val_loss: 1.2642 - val_acc: 0.7060\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.2263 - acc: 0.7212 - val_loss: 1.2597 - val_acc: 0.7060\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2219 - acc: 0.7219 - val_loss: 1.2553 - val_acc: 0.7060\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2190 - acc: 0.7208 - val_loss: 1.2530 - val_acc: 0.7070\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2158 - acc: 0.7220 - val_loss: 1.2518 - val_acc: 0.7070\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2127 - acc: 0.7213 - val_loss: 1.2508 - val_acc: 0.7090\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2095 - acc: 0.7232 - val_loss: 1.2452 - val_acc: 0.7100\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2058 - acc: 0.7224 - val_loss: 1.2420 - val_acc: 0.7060\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2025 - acc: 0.7249 - val_loss: 1.2383 - val_acc: 0.7110\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1998 - acc: 0.7233 - val_loss: 1.2349 - val_acc: 0.7100\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1961 - acc: 0.7252 - val_loss: 1.2308 - val_acc: 0.7100\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1930 - acc: 0.7232 - val_loss: 1.2336 - val_acc: 0.7010\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1907 - acc: 0.7240 - val_loss: 1.2256 - val_acc: 0.7090\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1874 - acc: 0.7243 - val_loss: 1.2210 - val_acc: 0.7130\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1842 - acc: 0.7253 - val_loss: 1.2248 - val_acc: 0.7040\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1817 - acc: 0.7265 - val_loss: 1.2216 - val_acc: 0.7150\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1790 - acc: 0.7240 - val_loss: 1.2159 - val_acc: 0.7110\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1759 - acc: 0.7267 - val_loss: 1.2157 - val_acc: 0.7110\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1733 - acc: 0.7256 - val_loss: 1.2136 - val_acc: 0.7180\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1710 - acc: 0.7257 - val_loss: 1.2101 - val_acc: 0.7130\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1676 - acc: 0.7275 - val_loss: 1.2043 - val_acc: 0.7120\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1649 - acc: 0.7261 - val_loss: 1.2046 - val_acc: 0.7200\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1627 - acc: 0.7280 - val_loss: 1.2024 - val_acc: 0.7120\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1597 - acc: 0.7281 - val_loss: 1.1963 - val_acc: 0.7170\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1575 - acc: 0.7272 - val_loss: 1.2020 - val_acc: 0.7130\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1547 - acc: 0.7272 - val_loss: 1.1946 - val_acc: 0.7160\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1525 - acc: 0.7292 - val_loss: 1.1929 - val_acc: 0.7140\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1500 - acc: 0.7296 - val_loss: 1.1899 - val_acc: 0.7130\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1475 - acc: 0.7279 - val_loss: 1.1852 - val_acc: 0.7190\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1450 - acc: 0.7305 - val_loss: 1.1869 - val_acc: 0.7140\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1433 - acc: 0.7293 - val_loss: 1.1811 - val_acc: 0.7140\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1405 - acc: 0.7289 - val_loss: 1.1814 - val_acc: 0.7130\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1385 - acc: 0.7296 - val_loss: 1.1771 - val_acc: 0.7180\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1367 - acc: 0.7311 - val_loss: 1.1760 - val_acc: 0.7180\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1340 - acc: 0.7296 - val_loss: 1.1732 - val_acc: 0.7210\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1318 - acc: 0.7312 - val_loss: 1.1751 - val_acc: 0.7200\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1297 - acc: 0.7315 - val_loss: 1.1694 - val_acc: 0.7200\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1277 - acc: 0.7317 - val_loss: 1.1700 - val_acc: 0.7180\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1259 - acc: 0.7309 - val_loss: 1.1661 - val_acc: 0.7170\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1240 - acc: 0.7313 - val_loss: 1.1670 - val_acc: 0.7220\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1220 - acc: 0.7321 - val_loss: 1.1615 - val_acc: 0.7210\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1200 - acc: 0.7315 - val_loss: 1.1599 - val_acc: 0.7180\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1181 - acc: 0.7321 - val_loss: 1.1583 - val_acc: 0.7180\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1159 - acc: 0.7337 - val_loss: 1.1594 - val_acc: 0.7190\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1143 - acc: 0.7341 - val_loss: 1.1539 - val_acc: 0.7200\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1128 - acc: 0.7324 - val_loss: 1.1557 - val_acc: 0.7210\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1107 - acc: 0.7337 - val_loss: 1.1598 - val_acc: 0.7200\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1091 - acc: 0.7356 - val_loss: 1.1567 - val_acc: 0.7220\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1077 - acc: 0.7345 - val_loss: 1.1486 - val_acc: 0.7250\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1059 - acc: 0.7343 - val_loss: 1.1469 - val_acc: 0.7240\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1034 - acc: 0.7357 - val_loss: 1.1450 - val_acc: 0.7260\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1023 - acc: 0.7360 - val_loss: 1.1498 - val_acc: 0.7170\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1003 - acc: 0.7353 - val_loss: 1.1464 - val_acc: 0.7270\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0991 - acc: 0.7357 - val_loss: 1.1429 - val_acc: 0.7220\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0970 - acc: 0.7371 - val_loss: 1.1390 - val_acc: 0.7240\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0958 - acc: 0.7367 - val_loss: 1.1397 - val_acc: 0.7260\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0933 - acc: 0.7373 - val_loss: 1.1406 - val_acc: 0.7210\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0927 - acc: 0.7361 - val_loss: 1.1328 - val_acc: 0.7290\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0907 - acc: 0.7377 - val_loss: 1.1374 - val_acc: 0.7260\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0896 - acc: 0.7367 - val_loss: 1.1338 - val_acc: 0.7280\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0877 - acc: 0.7361 - val_loss: 1.1337 - val_acc: 0.7240\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0859 - acc: 0.7349 - val_loss: 1.1300 - val_acc: 0.7240\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0845 - acc: 0.7364 - val_loss: 1.1291 - val_acc: 0.7320\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0833 - acc: 0.7373 - val_loss: 1.1257 - val_acc: 0.7260\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0815 - acc: 0.7372 - val_loss: 1.1257 - val_acc: 0.7340\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0802 - acc: 0.7372 - val_loss: 1.1247 - val_acc: 0.7210\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0790 - acc: 0.7368 - val_loss: 1.1229 - val_acc: 0.7300\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0770 - acc: 0.7395 - val_loss: 1.1210 - val_acc: 0.7290\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0755 - acc: 0.7377 - val_loss: 1.1316 - val_acc: 0.7230\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0745 - acc: 0.7391 - val_loss: 1.1207 - val_acc: 0.7300\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0735 - acc: 0.7385 - val_loss: 1.1178 - val_acc: 0.7320\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0710 - acc: 0.7397 - val_loss: 1.1155 - val_acc: 0.7320\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0697 - acc: 0.7395 - val_loss: 1.1165 - val_acc: 0.7350\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0692 - acc: 0.7387 - val_loss: 1.1186 - val_acc: 0.7360\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0673 - acc: 0.7403 - val_loss: 1.1134 - val_acc: 0.7300\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0656 - acc: 0.7400 - val_loss: 1.1146 - val_acc: 0.7230\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0650 - acc: 0.7404 - val_loss: 1.1127 - val_acc: 0.7320\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0630 - acc: 0.7401 - val_loss: 1.1102 - val_acc: 0.7260\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0619 - acc: 0.7416 - val_loss: 1.1078 - val_acc: 0.7290\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0609 - acc: 0.7421 - val_loss: 1.1058 - val_acc: 0.7320\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0596 - acc: 0.7400 - val_loss: 1.1056 - val_acc: 0.7350\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0591 - acc: 0.7405 - val_loss: 1.1043 - val_acc: 0.7330\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0574 - acc: 0.7411 - val_loss: 1.1025 - val_acc: 0.7300\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0562 - acc: 0.7399 - val_loss: 1.1031 - val_acc: 0.7350\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0544 - acc: 0.7404 - val_loss: 1.1089 - val_acc: 0.7230\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0531 - acc: 0.7435 - val_loss: 1.1084 - val_acc: 0.7250\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0529 - acc: 0.7417 - val_loss: 1.0992 - val_acc: 0.7310\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0505 - acc: 0.7411 - val_loss: 1.0974 - val_acc: 0.7360\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0493 - acc: 0.7432 - val_loss: 1.0954 - val_acc: 0.7350\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0492 - acc: 0.7419 - val_loss: 1.0971 - val_acc: 0.7320\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0479 - acc: 0.7420 - val_loss: 1.0937 - val_acc: 0.7360\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0460 - acc: 0.7445 - val_loss: 1.0962 - val_acc: 0.7370\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0447 - acc: 0.7439 - val_loss: 1.1022 - val_acc: 0.7280\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0438 - acc: 0.7443 - val_loss: 1.0909 - val_acc: 0.7340\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0423 - acc: 0.7437 - val_loss: 1.0898 - val_acc: 0.7310\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0418 - acc: 0.7435 - val_loss: 1.0899 - val_acc: 0.7340\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0399 - acc: 0.7429 - val_loss: 1.0941 - val_acc: 0.7270\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0391 - acc: 0.7433 - val_loss: 1.0993 - val_acc: 0.7210\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0387 - acc: 0.7444 - val_loss: 1.0864 - val_acc: 0.7410\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0370 - acc: 0.7453 - val_loss: 1.0876 - val_acc: 0.7360\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0361 - acc: 0.7452 - val_loss: 1.0860 - val_acc: 0.7400\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0347 - acc: 0.7457 - val_loss: 1.0851 - val_acc: 0.7400\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0333 - acc: 0.7459 - val_loss: 1.0848 - val_acc: 0.7300\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0326 - acc: 0.7436 - val_loss: 1.0822 - val_acc: 0.7360\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0316 - acc: 0.7467 - val_loss: 1.0808 - val_acc: 0.7420\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0303 - acc: 0.7467 - val_loss: 1.0801 - val_acc: 0.7360\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0298 - acc: 0.7456 - val_loss: 1.0794 - val_acc: 0.7370\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0287 - acc: 0.7469 - val_loss: 1.0787 - val_acc: 0.7390\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0276 - acc: 0.7469 - val_loss: 1.0766 - val_acc: 0.7380\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0270 - acc: 0.7455 - val_loss: 1.0776 - val_acc: 0.7400\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0253 - acc: 0.7451 - val_loss: 1.0756 - val_acc: 0.7400\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0244 - acc: 0.7469 - val_loss: 1.0734 - val_acc: 0.7420\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0234 - acc: 0.7468 - val_loss: 1.0726 - val_acc: 0.7380\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0220 - acc: 0.7456 - val_loss: 1.0754 - val_acc: 0.7340\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0216 - acc: 0.7476 - val_loss: 1.0712 - val_acc: 0.7400\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0202 - acc: 0.7483 - val_loss: 1.0747 - val_acc: 0.7420\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0195 - acc: 0.7473 - val_loss: 1.0758 - val_acc: 0.7360\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0184 - acc: 0.7469 - val_loss: 1.0701 - val_acc: 0.7360\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0180 - acc: 0.7463 - val_loss: 1.0755 - val_acc: 0.7270\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0170 - acc: 0.7492 - val_loss: 1.0675 - val_acc: 0.7420\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0163 - acc: 0.7483 - val_loss: 1.0676 - val_acc: 0.7330\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0149 - acc: 0.7492 - val_loss: 1.0672 - val_acc: 0.7340\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0142 - acc: 0.7501 - val_loss: 1.0745 - val_acc: 0.7330\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0137 - acc: 0.7499 - val_loss: 1.0693 - val_acc: 0.7410\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0126 - acc: 0.7495 - val_loss: 1.0651 - val_acc: 0.7420\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0115 - acc: 0.7487 - val_loss: 1.0633 - val_acc: 0.7400\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0103 - acc: 0.7500 - val_loss: 1.0623 - val_acc: 0.7420\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0098 - acc: 0.7492 - val_loss: 1.0645 - val_acc: 0.7390\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0092 - acc: 0.7508 - val_loss: 1.0614 - val_acc: 0.7390\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0083 - acc: 0.7511 - val_loss: 1.0631 - val_acc: 0.7420\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0079 - acc: 0.7485 - val_loss: 1.0628 - val_acc: 0.7360\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0072 - acc: 0.7503 - val_loss: 1.0591 - val_acc: 0.7420\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0064 - acc: 0.7491 - val_loss: 1.0618 - val_acc: 0.7380\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0055 - acc: 0.7524 - val_loss: 1.0584 - val_acc: 0.7410\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0035 - acc: 0.7512 - val_loss: 1.0617 - val_acc: 0.7390\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0042 - acc: 0.7511 - val_loss: 1.0621 - val_acc: 0.7440\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0023 - acc: 0.7504 - val_loss: 1.0582 - val_acc: 0.7400\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0017 - acc: 0.7511 - val_loss: 1.0600 - val_acc: 0.7370\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0017 - acc: 0.7529 - val_loss: 1.0630 - val_acc: 0.7360\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0002 - acc: 0.7515 - val_loss: 1.0626 - val_acc: 0.7370\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9996 - acc: 0.7527 - val_loss: 1.0586 - val_acc: 0.7420\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9989 - acc: 0.7519 - val_loss: 1.0573 - val_acc: 0.7410\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9978 - acc: 0.7527 - val_loss: 1.0521 - val_acc: 0.7450\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9969 - acc: 0.7524 - val_loss: 1.0554 - val_acc: 0.7370\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9964 - acc: 0.7515 - val_loss: 1.0520 - val_acc: 0.7350\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9955 - acc: 0.7527 - val_loss: 1.0499 - val_acc: 0.7410\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9952 - acc: 0.7521 - val_loss: 1.0531 - val_acc: 0.7410\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9937 - acc: 0.7536 - val_loss: 1.0503 - val_acc: 0.7430\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9934 - acc: 0.7527 - val_loss: 1.0488 - val_acc: 0.7410\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9928 - acc: 0.7540 - val_loss: 1.0482 - val_acc: 0.7370\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9922 - acc: 0.7531 - val_loss: 1.0513 - val_acc: 0.7460\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9918 - acc: 0.7532 - val_loss: 1.0469 - val_acc: 0.7430\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9906 - acc: 0.7537 - val_loss: 1.0485 - val_acc: 0.7440\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9899 - acc: 0.7535 - val_loss: 1.0490 - val_acc: 0.7430\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9892 - acc: 0.7528 - val_loss: 1.0456 - val_acc: 0.7450\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9885 - acc: 0.7553 - val_loss: 1.0468 - val_acc: 0.7430\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9873 - acc: 0.7533 - val_loss: 1.0522 - val_acc: 0.7330\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9875 - acc: 0.7544 - val_loss: 1.0438 - val_acc: 0.7400\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9860 - acc: 0.7532 - val_loss: 1.0437 - val_acc: 0.7370\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9858 - acc: 0.7552 - val_loss: 1.0509 - val_acc: 0.7350\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9852 - acc: 0.7572 - val_loss: 1.0473 - val_acc: 0.7380\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9849 - acc: 0.7545 - val_loss: 1.0505 - val_acc: 0.7360\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9833 - acc: 0.7571 - val_loss: 1.0404 - val_acc: 0.7370\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9835 - acc: 0.7544 - val_loss: 1.0396 - val_acc: 0.7440\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9824 - acc: 0.7555 - val_loss: 1.0394 - val_acc: 0.7470\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9807 - acc: 0.7571 - val_loss: 1.0390 - val_acc: 0.7390\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9812 - acc: 0.7565 - val_loss: 1.0413 - val_acc: 0.7370\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9800 - acc: 0.7564 - val_loss: 1.0424 - val_acc: 0.7370\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9802 - acc: 0.7567 - val_loss: 1.0416 - val_acc: 0.7440\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9796 - acc: 0.7575 - val_loss: 1.0395 - val_acc: 0.7380\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9785 - acc: 0.7588 - val_loss: 1.0391 - val_acc: 0.7470\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9780 - acc: 0.7573 - val_loss: 1.0413 - val_acc: 0.7390\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9773 - acc: 0.7592 - val_loss: 1.0395 - val_acc: 0.7340\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9768 - acc: 0.7584 - val_loss: 1.0367 - val_acc: 0.7430\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9759 - acc: 0.7583 - val_loss: 1.0386 - val_acc: 0.7360\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9757 - acc: 0.7587 - val_loss: 1.0356 - val_acc: 0.7440\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9748 - acc: 0.7599 - val_loss: 1.0374 - val_acc: 0.7450\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9748 - acc: 0.7604 - val_loss: 1.0375 - val_acc: 0.7450\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9733 - acc: 0.7588 - val_loss: 1.0436 - val_acc: 0.7340\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9740 - acc: 0.7584 - val_loss: 1.0363 - val_acc: 0.7400\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9727 - acc: 0.7600 - val_loss: 1.0417 - val_acc: 0.7360\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9720 - acc: 0.7616 - val_loss: 1.0353 - val_acc: 0.7390\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9723 - acc: 0.7589 - val_loss: 1.0331 - val_acc: 0.7390\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9714 - acc: 0.7613 - val_loss: 1.0309 - val_acc: 0.7450\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9707 - acc: 0.7591 - val_loss: 1.0358 - val_acc: 0.7500\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9702 - acc: 0.7613 - val_loss: 1.0385 - val_acc: 0.7440\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9703 - acc: 0.7595 - val_loss: 1.0309 - val_acc: 0.7430\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9694 - acc: 0.7592 - val_loss: 1.0327 - val_acc: 0.7440\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9690 - acc: 0.7613 - val_loss: 1.0355 - val_acc: 0.7440\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9681 - acc: 0.7607 - val_loss: 1.0419 - val_acc: 0.7430\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9671 - acc: 0.7625 - val_loss: 1.0296 - val_acc: 0.7450\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9665 - acc: 0.7607 - val_loss: 1.0324 - val_acc: 0.7440\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9661 - acc: 0.7601 - val_loss: 1.0334 - val_acc: 0.7400\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9662 - acc: 0.7648 - val_loss: 1.0325 - val_acc: 0.7440\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9657 - acc: 0.7605 - val_loss: 1.0275 - val_acc: 0.7440\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9647 - acc: 0.7607 - val_loss: 1.0303 - val_acc: 0.7420\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9638 - acc: 0.7604 - val_loss: 1.0320 - val_acc: 0.7400\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9633 - acc: 0.7621 - val_loss: 1.0248 - val_acc: 0.7430\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9628 - acc: 0.7596 - val_loss: 1.0360 - val_acc: 0.7390\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9635 - acc: 0.7620 - val_loss: 1.0289 - val_acc: 0.7440\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9620 - acc: 0.7640 - val_loss: 1.0362 - val_acc: 0.7350\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9619 - acc: 0.7624 - val_loss: 1.0283 - val_acc: 0.7450\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9624 - acc: 0.7604 - val_loss: 1.0283 - val_acc: 0.7440\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9611 - acc: 0.7611 - val_loss: 1.0373 - val_acc: 0.7420\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9612 - acc: 0.7623 - val_loss: 1.0356 - val_acc: 0.7370\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9606 - acc: 0.7639 - val_loss: 1.0278 - val_acc: 0.7350\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9599 - acc: 0.7629 - val_loss: 1.0301 - val_acc: 0.7410\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9595 - acc: 0.7639 - val_loss: 1.0253 - val_acc: 0.7410\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9584 - acc: 0.7640 - val_loss: 1.0224 - val_acc: 0.7480\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9579 - acc: 0.7639 - val_loss: 1.0263 - val_acc: 0.7450\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9578 - acc: 0.7648 - val_loss: 1.0273 - val_acc: 0.7400\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9572 - acc: 0.7628 - val_loss: 1.0273 - val_acc: 0.7420\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9571 - acc: 0.7643 - val_loss: 1.0213 - val_acc: 0.7460\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9556 - acc: 0.7652 - val_loss: 1.0288 - val_acc: 0.7480\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9556 - acc: 0.7653 - val_loss: 1.0252 - val_acc: 0.7410\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9551 - acc: 0.7659 - val_loss: 1.0212 - val_acc: 0.7450\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9547 - acc: 0.7665 - val_loss: 1.0225 - val_acc: 0.7440\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9538 - acc: 0.7659 - val_loss: 1.0207 - val_acc: 0.7430\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9541 - acc: 0.7653 - val_loss: 1.0211 - val_acc: 0.7470\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9538 - acc: 0.7652 - val_loss: 1.0187 - val_acc: 0.7430\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9520 - acc: 0.7643 - val_loss: 1.0242 - val_acc: 0.7420\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9529 - acc: 0.7679 - val_loss: 1.0203 - val_acc: 0.7440\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9516 - acc: 0.7656 - val_loss: 1.0341 - val_acc: 0.7420\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9515 - acc: 0.7657 - val_loss: 1.0174 - val_acc: 0.7400\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9514 - acc: 0.7687 - val_loss: 1.0186 - val_acc: 0.7440\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9505 - acc: 0.7669 - val_loss: 1.0220 - val_acc: 0.7460\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9510 - acc: 0.7660 - val_loss: 1.0184 - val_acc: 0.7470\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9500 - acc: 0.7669 - val_loss: 1.0157 - val_acc: 0.7460\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9494 - acc: 0.7653 - val_loss: 1.0273 - val_acc: 0.7380\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9491 - acc: 0.7677 - val_loss: 1.0353 - val_acc: 0.7360\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9496 - acc: 0.7680 - val_loss: 1.0145 - val_acc: 0.7410\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9481 - acc: 0.7668 - val_loss: 1.0180 - val_acc: 0.7440\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9477 - acc: 0.7651 - val_loss: 1.0339 - val_acc: 0.7430\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9493 - acc: 0.7680 - val_loss: 1.0158 - val_acc: 0.7430\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9470 - acc: 0.7665 - val_loss: 1.0199 - val_acc: 0.7460\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9473 - acc: 0.7684 - val_loss: 1.0143 - val_acc: 0.7460\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9463 - acc: 0.7684 - val_loss: 1.0177 - val_acc: 0.7430\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9467 - acc: 0.7668 - val_loss: 1.0147 - val_acc: 0.7460\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9451 - acc: 0.7703 - val_loss: 1.0189 - val_acc: 0.7450\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9453 - acc: 0.7696 - val_loss: 1.0192 - val_acc: 0.7450\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9455 - acc: 0.7693 - val_loss: 1.0142 - val_acc: 0.7470\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9449 - acc: 0.7696 - val_loss: 1.0135 - val_acc: 0.7460\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9446 - acc: 0.7692 - val_loss: 1.0128 - val_acc: 0.7460\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9431 - acc: 0.7700 - val_loss: 1.0204 - val_acc: 0.7460\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9440 - acc: 0.7679 - val_loss: 1.0129 - val_acc: 0.7490\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9428 - acc: 0.7724 - val_loss: 1.0149 - val_acc: 0.7470\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9431 - acc: 0.7683 - val_loss: 1.0139 - val_acc: 0.7460\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9418 - acc: 0.7687 - val_loss: 1.0224 - val_acc: 0.7340\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9415 - acc: 0.7693 - val_loss: 1.0143 - val_acc: 0.7390\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9414 - acc: 0.7697 - val_loss: 1.0124 - val_acc: 0.7460\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9419 - acc: 0.7691 - val_loss: 1.0148 - val_acc: 0.7450\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9406 - acc: 0.7673 - val_loss: 1.0105 - val_acc: 0.7490\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9407 - acc: 0.7716 - val_loss: 1.0115 - val_acc: 0.7440\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9410 - acc: 0.7727 - val_loss: 1.0139 - val_acc: 0.7450\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9392 - acc: 0.7715 - val_loss: 1.0098 - val_acc: 0.7450\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9387 - acc: 0.7712 - val_loss: 1.0085 - val_acc: 0.7460\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9397 - acc: 0.7723 - val_loss: 1.0188 - val_acc: 0.7380\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9391 - acc: 0.7720 - val_loss: 1.0171 - val_acc: 0.7490\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9384 - acc: 0.7711 - val_loss: 1.0181 - val_acc: 0.7490\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9384 - acc: 0.7689 - val_loss: 1.0161 - val_acc: 0.7480\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9374 - acc: 0.7721 - val_loss: 1.0109 - val_acc: 0.7450\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9377 - acc: 0.7715 - val_loss: 1.0146 - val_acc: 0.7490\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9378 - acc: 0.7725 - val_loss: 1.0141 - val_acc: 0.7470\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9368 - acc: 0.7711 - val_loss: 1.0096 - val_acc: 0.7460\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9360 - acc: 0.7733 - val_loss: 1.0100 - val_acc: 0.7360\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9358 - acc: 0.7708 - val_loss: 1.0100 - val_acc: 0.7420\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9352 - acc: 0.7735 - val_loss: 1.0219 - val_acc: 0.7370\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9356 - acc: 0.7707 - val_loss: 1.0082 - val_acc: 0.7470\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9362 - acc: 0.7703 - val_loss: 1.0132 - val_acc: 0.7420\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9349 - acc: 0.7721 - val_loss: 1.0112 - val_acc: 0.7490\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9346 - acc: 0.7741 - val_loss: 1.0167 - val_acc: 0.7440\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9334 - acc: 0.7748 - val_loss: 1.0089 - val_acc: 0.7440\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9331 - acc: 0.7733 - val_loss: 1.0065 - val_acc: 0.7460\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9338 - acc: 0.7747 - val_loss: 1.0074 - val_acc: 0.7450\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9322 - acc: 0.7725 - val_loss: 1.0119 - val_acc: 0.7390\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9330 - acc: 0.7715 - val_loss: 1.0081 - val_acc: 0.7450\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9329 - acc: 0.7737 - val_loss: 1.0065 - val_acc: 0.7470\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9321 - acc: 0.7731 - val_loss: 1.0092 - val_acc: 0.7470\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9326 - acc: 0.7744 - val_loss: 1.0094 - val_acc: 0.7470\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9309 - acc: 0.7740 - val_loss: 1.0120 - val_acc: 0.7430\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9309 - acc: 0.7705 - val_loss: 1.0067 - val_acc: 0.7520\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9299 - acc: 0.7741 - val_loss: 1.0117 - val_acc: 0.7410\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9299 - acc: 0.7739 - val_loss: 1.0045 - val_acc: 0.7480\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9283 - acc: 0.7729 - val_loss: 1.0092 - val_acc: 0.7430\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9300 - acc: 0.7757 - val_loss: 1.0071 - val_acc: 0.7490\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9287 - acc: 0.7773 - val_loss: 1.0077 - val_acc: 0.7440\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9283 - acc: 0.7748 - val_loss: 1.0034 - val_acc: 0.7470\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9293 - acc: 0.7743 - val_loss: 1.0136 - val_acc: 0.7460\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9276 - acc: 0.7767 - val_loss: 1.0055 - val_acc: 0.7430\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9273 - acc: 0.7772 - val_loss: 1.0035 - val_acc: 0.7440\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9285 - acc: 0.7768 - val_loss: 1.0046 - val_acc: 0.7490\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9285 - acc: 0.7751 - val_loss: 1.0053 - val_acc: 0.7470\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9267 - acc: 0.7759 - val_loss: 1.0108 - val_acc: 0.7470\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9269 - acc: 0.7772 - val_loss: 1.0013 - val_acc: 0.7490\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9255 - acc: 0.7752 - val_loss: 1.0125 - val_acc: 0.7420\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9269 - acc: 0.7737 - val_loss: 1.0041 - val_acc: 0.7490\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9259 - acc: 0.7791 - val_loss: 1.0013 - val_acc: 0.7500\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9248 - acc: 0.7775 - val_loss: 1.0076 - val_acc: 0.7430\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9252 - acc: 0.7749 - val_loss: 1.0238 - val_acc: 0.7360\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9258 - acc: 0.7747 - val_loss: 1.0041 - val_acc: 0.7460\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9242 - acc: 0.7753 - val_loss: 1.0106 - val_acc: 0.7470\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9248 - acc: 0.7779 - val_loss: 1.0083 - val_acc: 0.7440\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9237 - acc: 0.7784 - val_loss: 1.0022 - val_acc: 0.7520\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9237 - acc: 0.7780 - val_loss: 0.9996 - val_acc: 0.7500\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9236 - acc: 0.7795 - val_loss: 1.0051 - val_acc: 0.7470\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9230 - acc: 0.7775 - val_loss: 1.0005 - val_acc: 0.7520\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9218 - acc: 0.7757 - val_loss: 1.0026 - val_acc: 0.7480\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9224 - acc: 0.7780 - val_loss: 0.9992 - val_acc: 0.7470\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9233 - acc: 0.7769 - val_loss: 1.0095 - val_acc: 0.7500\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9224 - acc: 0.7773 - val_loss: 0.9962 - val_acc: 0.7480\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9207 - acc: 0.7781 - val_loss: 1.0070 - val_acc: 0.7440\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9213 - acc: 0.7780 - val_loss: 0.9979 - val_acc: 0.7450\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9208 - acc: 0.7757 - val_loss: 0.9983 - val_acc: 0.7450\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9207 - acc: 0.7780 - val_loss: 1.0028 - val_acc: 0.7470\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9197 - acc: 0.7755 - val_loss: 0.9995 - val_acc: 0.7490\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9197 - acc: 0.7793 - val_loss: 0.9986 - val_acc: 0.7460\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9190 - acc: 0.7769 - val_loss: 1.0099 - val_acc: 0.7440\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9208 - acc: 0.7765 - val_loss: 1.0078 - val_acc: 0.7470\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9183 - acc: 0.7803 - val_loss: 0.9999 - val_acc: 0.7470\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9195 - acc: 0.7809 - val_loss: 0.9982 - val_acc: 0.7480\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9187 - acc: 0.7776 - val_loss: 0.9983 - val_acc: 0.7490\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9178 - acc: 0.7793 - val_loss: 1.0044 - val_acc: 0.7480\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9174 - acc: 0.7788 - val_loss: 1.0074 - val_acc: 0.7470\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9183 - acc: 0.7788 - val_loss: 1.0005 - val_acc: 0.7480\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9169 - acc: 0.7811 - val_loss: 1.0033 - val_acc: 0.7470\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9179 - acc: 0.7764 - val_loss: 0.9949 - val_acc: 0.7490\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9161 - acc: 0.7799 - val_loss: 0.9985 - val_acc: 0.7480\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9162 - acc: 0.7827 - val_loss: 0.9983 - val_acc: 0.7500\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9160 - acc: 0.7813 - val_loss: 0.9977 - val_acc: 0.7490\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9155 - acc: 0.7811 - val_loss: 1.0011 - val_acc: 0.7460\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9151 - acc: 0.7827 - val_loss: 0.9946 - val_acc: 0.7510\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9144 - acc: 0.7811 - val_loss: 0.9990 - val_acc: 0.7460\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9141 - acc: 0.7804 - val_loss: 0.9984 - val_acc: 0.7490\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9155 - acc: 0.7805 - val_loss: 0.9937 - val_acc: 0.7480\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9136 - acc: 0.7813 - val_loss: 0.9985 - val_acc: 0.7480\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9136 - acc: 0.7791 - val_loss: 0.9951 - val_acc: 0.7510\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9127 - acc: 0.7816 - val_loss: 0.9966 - val_acc: 0.7450\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9124 - acc: 0.7824 - val_loss: 1.0013 - val_acc: 0.7490\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9136 - acc: 0.7809 - val_loss: 0.9961 - val_acc: 0.7500\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9126 - acc: 0.7805 - val_loss: 1.0021 - val_acc: 0.7420\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9119 - acc: 0.7819 - val_loss: 0.9945 - val_acc: 0.7480\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9126 - acc: 0.7821 - val_loss: 0.9935 - val_acc: 0.7490\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9121 - acc: 0.7797 - val_loss: 0.9904 - val_acc: 0.7490\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9112 - acc: 0.7825 - val_loss: 0.9998 - val_acc: 0.7450\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9116 - acc: 0.7813 - val_loss: 1.0023 - val_acc: 0.7500\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9110 - acc: 0.7820 - val_loss: 0.9928 - val_acc: 0.7510\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9102 - acc: 0.7827 - val_loss: 1.0061 - val_acc: 0.7430\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9114 - acc: 0.7837 - val_loss: 0.9934 - val_acc: 0.7490\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9099 - acc: 0.7833 - val_loss: 0.9906 - val_acc: 0.7510\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9081 - acc: 0.7827 - val_loss: 0.9915 - val_acc: 0.7500\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9093 - acc: 0.7821 - val_loss: 0.9899 - val_acc: 0.7480\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9088 - acc: 0.7833 - val_loss: 0.9912 - val_acc: 0.7530\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9085 - acc: 0.7844 - val_loss: 0.9929 - val_acc: 0.7490\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9076 - acc: 0.7832 - val_loss: 0.9948 - val_acc: 0.7500\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9082 - acc: 0.7845 - val_loss: 0.9931 - val_acc: 0.7480\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9078 - acc: 0.7860 - val_loss: 0.9939 - val_acc: 0.7450\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9082 - acc: 0.7863 - val_loss: 0.9884 - val_acc: 0.7490\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9074 - acc: 0.7831 - val_loss: 0.9919 - val_acc: 0.7490\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9072 - acc: 0.7836 - val_loss: 0.9933 - val_acc: 0.7470\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9061 - acc: 0.7845 - val_loss: 0.9890 - val_acc: 0.7520\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9055 - acc: 0.7849 - val_loss: 0.9951 - val_acc: 0.7480\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9069 - acc: 0.7815 - val_loss: 0.9892 - val_acc: 0.7500\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9056 - acc: 0.7845 - val_loss: 0.9898 - val_acc: 0.7450\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9055 - acc: 0.7837 - val_loss: 1.0033 - val_acc: 0.7410\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9059 - acc: 0.7844 - val_loss: 0.9874 - val_acc: 0.7490\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9055 - acc: 0.7828 - val_loss: 0.9868 - val_acc: 0.7510\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9041 - acc: 0.7853 - val_loss: 0.9986 - val_acc: 0.7460\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9044 - acc: 0.7852 - val_loss: 0.9906 - val_acc: 0.7510\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9051 - acc: 0.7848 - val_loss: 1.0004 - val_acc: 0.7480\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9045 - acc: 0.7829 - val_loss: 0.9890 - val_acc: 0.7480\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9029 - acc: 0.7868 - val_loss: 0.9888 - val_acc: 0.7500\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9041 - acc: 0.7848 - val_loss: 0.9849 - val_acc: 0.7490\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9027 - acc: 0.7863 - val_loss: 0.9917 - val_acc: 0.7480\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9034 - acc: 0.7833 - val_loss: 0.9846 - val_acc: 0.7520\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9023 - acc: 0.7851 - val_loss: 0.9914 - val_acc: 0.7520\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9020 - acc: 0.7868 - val_loss: 0.9894 - val_acc: 0.7500\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9020 - acc: 0.7847 - val_loss: 0.9931 - val_acc: 0.7510\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9020 - acc: 0.7859 - val_loss: 0.9889 - val_acc: 0.7520\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9014 - acc: 0.7837 - val_loss: 0.9999 - val_acc: 0.7400\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9026 - acc: 0.7857 - val_loss: 0.9892 - val_acc: 0.7490\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9020 - acc: 0.7848 - val_loss: 0.9958 - val_acc: 0.7450\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9008 - acc: 0.7852 - val_loss: 0.9971 - val_acc: 0.7430\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9020 - acc: 0.7837 - val_loss: 1.0106 - val_acc: 0.7420\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9013 - acc: 0.7873 - val_loss: 0.9923 - val_acc: 0.7410\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8989 - acc: 0.7867 - val_loss: 0.9989 - val_acc: 0.7460\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9006 - acc: 0.7860 - val_loss: 0.9994 - val_acc: 0.7460\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9007 - acc: 0.7855 - val_loss: 0.9868 - val_acc: 0.7510\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8998 - acc: 0.7872 - val_loss: 0.9989 - val_acc: 0.7550\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8999 - acc: 0.7852 - val_loss: 0.9917 - val_acc: 0.7480\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8977 - acc: 0.7885 - val_loss: 0.9837 - val_acc: 0.7520\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8984 - acc: 0.7884 - val_loss: 0.9920 - val_acc: 0.7510\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8985 - acc: 0.7860 - val_loss: 0.9832 - val_acc: 0.7550\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8976 - acc: 0.7879 - val_loss: 0.9873 - val_acc: 0.7510\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8976 - acc: 0.7883 - val_loss: 0.9904 - val_acc: 0.7460\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8973 - acc: 0.7884 - val_loss: 0.9906 - val_acc: 0.7490\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8966 - acc: 0.7900 - val_loss: 0.9852 - val_acc: 0.7490\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8971 - acc: 0.7875 - val_loss: 0.9865 - val_acc: 0.7500\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8975 - acc: 0.7891 - val_loss: 0.9849 - val_acc: 0.7540\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8960 - acc: 0.7904 - val_loss: 0.9915 - val_acc: 0.7440\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8969 - acc: 0.7912 - val_loss: 0.9867 - val_acc: 0.7520\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8960 - acc: 0.7875 - val_loss: 0.9846 - val_acc: 0.7510\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8954 - acc: 0.7899 - val_loss: 0.9949 - val_acc: 0.7570\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8951 - acc: 0.7872 - val_loss: 0.9797 - val_acc: 0.7550\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8954 - acc: 0.7888 - val_loss: 0.9809 - val_acc: 0.7480\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8951 - acc: 0.7861 - val_loss: 0.9815 - val_acc: 0.7540\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8949 - acc: 0.7855 - val_loss: 0.9820 - val_acc: 0.7510\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8949 - acc: 0.7909 - val_loss: 0.9838 - val_acc: 0.7460\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8942 - acc: 0.7893 - val_loss: 0.9807 - val_acc: 0.7520\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8936 - acc: 0.7907 - val_loss: 0.9870 - val_acc: 0.7510\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8930 - acc: 0.7907 - val_loss: 1.0098 - val_acc: 0.7490\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8928 - acc: 0.7897 - val_loss: 0.9818 - val_acc: 0.7470\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8934 - acc: 0.7872 - val_loss: 0.9907 - val_acc: 0.7440\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8934 - acc: 0.7872 - val_loss: 0.9817 - val_acc: 0.7490\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8927 - acc: 0.7883 - val_loss: 0.9817 - val_acc: 0.7500\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8915 - acc: 0.7899 - val_loss: 0.9947 - val_acc: 0.7470\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8909 - acc: 0.7907 - val_loss: 0.9824 - val_acc: 0.7570\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8907 - acc: 0.7928 - val_loss: 0.9905 - val_acc: 0.7460\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8931 - acc: 0.7893 - val_loss: 1.0090 - val_acc: 0.7380\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8939 - acc: 0.7864 - val_loss: 0.9812 - val_acc: 0.7530\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8906 - acc: 0.7908 - val_loss: 0.9841 - val_acc: 0.7510\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8915 - acc: 0.7917 - val_loss: 0.9862 - val_acc: 0.7470\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8906 - acc: 0.7903 - val_loss: 0.9799 - val_acc: 0.7530\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8898 - acc: 0.7904 - val_loss: 0.9841 - val_acc: 0.7540\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8909 - acc: 0.7880 - val_loss: 0.9931 - val_acc: 0.7450\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8898 - acc: 0.7933 - val_loss: 1.0073 - val_acc: 0.7540\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8906 - acc: 0.7891 - val_loss: 0.9849 - val_acc: 0.7430\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8896 - acc: 0.7909 - val_loss: 0.9854 - val_acc: 0.7440\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8901 - acc: 0.7920 - val_loss: 0.9790 - val_acc: 0.7570\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8893 - acc: 0.7915 - val_loss: 0.9814 - val_acc: 0.7510\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8896 - acc: 0.7903 - val_loss: 0.9771 - val_acc: 0.7540\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8889 - acc: 0.7900 - val_loss: 0.9917 - val_acc: 0.7440\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8888 - acc: 0.7909 - val_loss: 1.0060 - val_acc: 0.7380\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8883 - acc: 0.7927 - val_loss: 0.9883 - val_acc: 0.7510\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8883 - acc: 0.7915 - val_loss: 0.9801 - val_acc: 0.7540\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8871 - acc: 0.7928 - val_loss: 0.9815 - val_acc: 0.7520\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8889 - acc: 0.7912 - val_loss: 0.9791 - val_acc: 0.7500\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8872 - acc: 0.7907 - val_loss: 0.9750 - val_acc: 0.7560\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8858 - acc: 0.7935 - val_loss: 0.9761 - val_acc: 0.7500\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8870 - acc: 0.7937 - val_loss: 0.9850 - val_acc: 0.7520\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8865 - acc: 0.7948 - val_loss: 0.9804 - val_acc: 0.7550\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8861 - acc: 0.7944 - val_loss: 0.9749 - val_acc: 0.7530\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8867 - acc: 0.7908 - val_loss: 0.9772 - val_acc: 0.7510\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8844 - acc: 0.7931 - val_loss: 0.9822 - val_acc: 0.7490\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8856 - acc: 0.7915 - val_loss: 0.9743 - val_acc: 0.7560\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8848 - acc: 0.7916 - val_loss: 0.9847 - val_acc: 0.7500\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8855 - acc: 0.7923 - val_loss: 0.9855 - val_acc: 0.7490\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8852 - acc: 0.7956 - val_loss: 0.9773 - val_acc: 0.7560\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8841 - acc: 0.7949 - val_loss: 0.9808 - val_acc: 0.7600\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8834 - acc: 0.7933 - val_loss: 0.9722 - val_acc: 0.7600\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8846 - acc: 0.7916 - val_loss: 0.9740 - val_acc: 0.7550\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8836 - acc: 0.7932 - val_loss: 1.0125 - val_acc: 0.7370\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8839 - acc: 0.7933 - val_loss: 0.9815 - val_acc: 0.7510\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8844 - acc: 0.7931 - val_loss: 1.0180 - val_acc: 0.7400\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8834 - acc: 0.7925 - val_loss: 0.9715 - val_acc: 0.7600\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8823 - acc: 0.7929 - val_loss: 0.9744 - val_acc: 0.7520\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8830 - acc: 0.7921 - val_loss: 0.9799 - val_acc: 0.7510\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8838 - acc: 0.7955 - val_loss: 0.9798 - val_acc: 0.7550\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8817 - acc: 0.7949 - val_loss: 0.9786 - val_acc: 0.7500\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8813 - acc: 0.7919 - val_loss: 0.9711 - val_acc: 0.7550\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8818 - acc: 0.7941 - val_loss: 0.9803 - val_acc: 0.7490\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8813 - acc: 0.7925 - val_loss: 0.9741 - val_acc: 0.7590\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8817 - acc: 0.7921 - val_loss: 0.9830 - val_acc: 0.7520\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8804 - acc: 0.7939 - val_loss: 0.9770 - val_acc: 0.7590\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8801 - acc: 0.7965 - val_loss: 0.9743 - val_acc: 0.7500\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8796 - acc: 0.7948 - val_loss: 0.9938 - val_acc: 0.7400\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8817 - acc: 0.7945 - val_loss: 0.9749 - val_acc: 0.7520\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8800 - acc: 0.7956 - val_loss: 0.9799 - val_acc: 0.7530\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8795 - acc: 0.7949 - val_loss: 0.9829 - val_acc: 0.7460\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8811 - acc: 0.7951 - val_loss: 0.9728 - val_acc: 0.7520\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8793 - acc: 0.7969 - val_loss: 0.9706 - val_acc: 0.7540\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8798 - acc: 0.7936 - val_loss: 0.9752 - val_acc: 0.7530\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8793 - acc: 0.7955 - val_loss: 0.9663 - val_acc: 0.7570\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8778 - acc: 0.7956 - val_loss: 0.9720 - val_acc: 0.7550\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8767 - acc: 0.7949 - val_loss: 0.9773 - val_acc: 0.7550\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8794 - acc: 0.7933 - val_loss: 0.9746 - val_acc: 0.7470\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8772 - acc: 0.7955 - val_loss: 0.9692 - val_acc: 0.7620\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8779 - acc: 0.7945 - val_loss: 0.9701 - val_acc: 0.7570\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8778 - acc: 0.7963 - val_loss: 0.9917 - val_acc: 0.7550\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8791 - acc: 0.7969 - val_loss: 0.9712 - val_acc: 0.7500\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8767 - acc: 0.7969 - val_loss: 0.9719 - val_acc: 0.7590\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8773 - acc: 0.7963 - val_loss: 0.9750 - val_acc: 0.7490\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8773 - acc: 0.7964 - val_loss: 0.9703 - val_acc: 0.7570\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8752 - acc: 0.7980 - val_loss: 0.9786 - val_acc: 0.7510\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8769 - acc: 0.7968 - val_loss: 0.9772 - val_acc: 0.7510\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8756 - acc: 0.8001 - val_loss: 1.0021 - val_acc: 0.7350\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8784 - acc: 0.7940 - val_loss: 0.9881 - val_acc: 0.7480\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8743 - acc: 0.7981 - val_loss: 0.9705 - val_acc: 0.7530\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8735 - acc: 0.7999 - val_loss: 0.9827 - val_acc: 0.7520\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8756 - acc: 0.7972 - val_loss: 0.9711 - val_acc: 0.7590\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8745 - acc: 0.7997 - val_loss: 0.9684 - val_acc: 0.7540\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8739 - acc: 0.7975 - val_loss: 0.9685 - val_acc: 0.7570\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8738 - acc: 0.8011 - val_loss: 0.9674 - val_acc: 0.7530\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8749 - acc: 0.7981 - val_loss: 0.9672 - val_acc: 0.7610\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8744 - acc: 0.7976 - val_loss: 0.9682 - val_acc: 0.7550\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8727 - acc: 0.7992 - val_loss: 0.9878 - val_acc: 0.7460\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8733 - acc: 0.7979 - val_loss: 0.9775 - val_acc: 0.7530\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8737 - acc: 0.7975 - val_loss: 0.9645 - val_acc: 0.7560\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8727 - acc: 0.7988 - val_loss: 0.9718 - val_acc: 0.7540\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8722 - acc: 0.8003 - val_loss: 0.9634 - val_acc: 0.7640\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8716 - acc: 0.7985 - val_loss: 0.9827 - val_acc: 0.7470\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8716 - acc: 0.8000 - val_loss: 0.9694 - val_acc: 0.7600\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8706 - acc: 0.7991 - val_loss: 0.9899 - val_acc: 0.7380\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8725 - acc: 0.7976 - val_loss: 0.9677 - val_acc: 0.7550\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8723 - acc: 0.8001 - val_loss: 0.9892 - val_acc: 0.7530\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8716 - acc: 0.7995 - val_loss: 0.9662 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8710 - acc: 0.7997 - val_loss: 0.9686 - val_acc: 0.7610\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8693 - acc: 0.7977 - val_loss: 0.9673 - val_acc: 0.7530\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8707 - acc: 0.7992 - val_loss: 0.9608 - val_acc: 0.7590\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8703 - acc: 0.7985 - val_loss: 1.0115 - val_acc: 0.7520\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8703 - acc: 0.7999 - val_loss: 0.9695 - val_acc: 0.7520\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8699 - acc: 0.7968 - val_loss: 0.9745 - val_acc: 0.7500\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8707 - acc: 0.7993 - val_loss: 0.9641 - val_acc: 0.7580\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8680 - acc: 0.8011 - val_loss: 0.9653 - val_acc: 0.7630\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8667 - acc: 0.7981 - val_loss: 0.9663 - val_acc: 0.7580\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8689 - acc: 0.7996 - val_loss: 0.9614 - val_acc: 0.7590\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8687 - acc: 0.7992 - val_loss: 0.9748 - val_acc: 0.7530\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8670 - acc: 0.7988 - val_loss: 0.9627 - val_acc: 0.7590\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8683 - acc: 0.8008 - val_loss: 0.9633 - val_acc: 0.7620\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8670 - acc: 0.7993 - val_loss: 0.9732 - val_acc: 0.7490\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8676 - acc: 0.7989 - val_loss: 0.9659 - val_acc: 0.7520\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8668 - acc: 0.8023 - val_loss: 0.9720 - val_acc: 0.7480\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8659 - acc: 0.8004 - val_loss: 0.9639 - val_acc: 0.7640\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8661 - acc: 0.7996 - val_loss: 0.9690 - val_acc: 0.7590\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8668 - acc: 0.8012 - val_loss: 0.9654 - val_acc: 0.7600\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8646 - acc: 0.8015 - val_loss: 0.9665 - val_acc: 0.7610\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8655 - acc: 0.8005 - val_loss: 0.9681 - val_acc: 0.7520\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8637 - acc: 0.8007 - val_loss: 0.9586 - val_acc: 0.7600\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8649 - acc: 0.8008 - val_loss: 0.9716 - val_acc: 0.7580\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8650 - acc: 0.7989 - val_loss: 0.9620 - val_acc: 0.7570\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8649 - acc: 0.8013 - val_loss: 0.9624 - val_acc: 0.7560\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8650 - acc: 0.8016 - val_loss: 0.9603 - val_acc: 0.7570\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8652 - acc: 0.8015 - val_loss: 0.9589 - val_acc: 0.7650\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8636 - acc: 0.8008 - val_loss: 0.9590 - val_acc: 0.7570\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8637 - acc: 0.8025 - val_loss: 0.9615 - val_acc: 0.7550\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8627 - acc: 0.8011 - val_loss: 0.9623 - val_acc: 0.7600\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8647 - acc: 0.8005 - val_loss: 0.9786 - val_acc: 0.7550\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8644 - acc: 0.8037 - val_loss: 0.9685 - val_acc: 0.7560\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8620 - acc: 0.8005 - val_loss: 0.9617 - val_acc: 0.7540\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8636 - acc: 0.8000 - val_loss: 0.9750 - val_acc: 0.7590\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8633 - acc: 0.8033 - val_loss: 0.9559 - val_acc: 0.7620\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8614 - acc: 0.8001 - val_loss: 0.9696 - val_acc: 0.7610\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8621 - acc: 0.8029 - val_loss: 0.9621 - val_acc: 0.7580\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8630 - acc: 0.7988 - val_loss: 0.9657 - val_acc: 0.7580\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8613 - acc: 0.8021 - val_loss: 0.9574 - val_acc: 0.7610\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8609 - acc: 0.8031 - val_loss: 0.9657 - val_acc: 0.7490\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8632 - acc: 0.8027 - val_loss: 0.9645 - val_acc: 0.7490\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8598 - acc: 0.8033 - val_loss: 0.9631 - val_acc: 0.7570\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8602 - acc: 0.8029 - val_loss: 0.9763 - val_acc: 0.7500\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8623 - acc: 0.8007 - val_loss: 0.9525 - val_acc: 0.7650\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8601 - acc: 0.8045 - val_loss: 0.9631 - val_acc: 0.7590\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8591 - acc: 0.8035 - val_loss: 0.9722 - val_acc: 0.7480\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8593 - acc: 0.8012 - val_loss: 0.9561 - val_acc: 0.7650\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8587 - acc: 0.8027 - val_loss: 0.9549 - val_acc: 0.7640\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8594 - acc: 0.8005 - val_loss: 0.9608 - val_acc: 0.7590\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8588 - acc: 0.8040 - val_loss: 0.9580 - val_acc: 0.7540\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8593 - acc: 0.8011 - val_loss: 0.9549 - val_acc: 0.7640\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8592 - acc: 0.8020 - val_loss: 0.9541 - val_acc: 0.7660\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8600 - acc: 0.8047 - val_loss: 0.9895 - val_acc: 0.7460\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8586 - acc: 0.8055 - val_loss: 0.9568 - val_acc: 0.7640\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8593 - acc: 0.8036 - val_loss: 0.9774 - val_acc: 0.7450\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8561 - acc: 0.8059 - val_loss: 0.9789 - val_acc: 0.7580\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8582 - acc: 0.8031 - val_loss: 0.9639 - val_acc: 0.7640\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8574 - acc: 0.8023 - val_loss: 0.9580 - val_acc: 0.7650\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8581 - acc: 0.8021 - val_loss: 0.9685 - val_acc: 0.7590\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8575 - acc: 0.8040 - val_loss: 0.9534 - val_acc: 0.7630\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8570 - acc: 0.8025 - val_loss: 0.9586 - val_acc: 0.7610\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8562 - acc: 0.8005 - val_loss: 0.9552 - val_acc: 0.7640\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8557 - acc: 0.8064 - val_loss: 0.9556 - val_acc: 0.7590\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8550 - acc: 0.8024 - val_loss: 0.9606 - val_acc: 0.7620\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8542 - acc: 0.8037 - val_loss: 0.9604 - val_acc: 0.7650\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8558 - acc: 0.8031 - val_loss: 0.9786 - val_acc: 0.7530\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8571 - acc: 0.8015 - val_loss: 0.9588 - val_acc: 0.7610\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8554 - acc: 0.8029 - val_loss: 0.9564 - val_acc: 0.7610\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8529 - acc: 0.8060 - val_loss: 0.9566 - val_acc: 0.7550\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8544 - acc: 0.8061 - val_loss: 0.9649 - val_acc: 0.7610\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8541 - acc: 0.8055 - val_loss: 0.9671 - val_acc: 0.7570\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8548 - acc: 0.8064 - val_loss: 0.9627 - val_acc: 0.7470\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8550 - acc: 0.8055 - val_loss: 0.9627 - val_acc: 0.7610\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8549 - acc: 0.8076 - val_loss: 0.9503 - val_acc: 0.7640\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8537 - acc: 0.8053 - val_loss: 0.9546 - val_acc: 0.7570\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8520 - acc: 0.8067 - val_loss: 0.9514 - val_acc: 0.7610\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8517 - acc: 0.8065 - val_loss: 0.9556 - val_acc: 0.7570\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8515 - acc: 0.8049 - val_loss: 0.9490 - val_acc: 0.7660\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8522 - acc: 0.8076 - val_loss: 0.9766 - val_acc: 0.7660\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8517 - acc: 0.8055 - val_loss: 0.9534 - val_acc: 0.7580\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8538 - acc: 0.8057 - val_loss: 0.9573 - val_acc: 0.7610\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8503 - acc: 0.8076 - val_loss: 0.9496 - val_acc: 0.7680\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8514 - acc: 0.8031 - val_loss: 0.9634 - val_acc: 0.7620\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8526 - acc: 0.8048 - val_loss: 0.9494 - val_acc: 0.7680\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8530 - acc: 0.8037 - val_loss: 0.9548 - val_acc: 0.7650\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8523 - acc: 0.8045 - val_loss: 0.9587 - val_acc: 0.7480\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8524 - acc: 0.8032 - val_loss: 0.9567 - val_acc: 0.7550\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8505 - acc: 0.8077 - val_loss: 0.9572 - val_acc: 0.7610\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8495 - acc: 0.8081 - val_loss: 0.9509 - val_acc: 0.7630\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8504 - acc: 0.8039 - val_loss: 0.9539 - val_acc: 0.7540\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8499 - acc: 0.8068 - val_loss: 0.9651 - val_acc: 0.7530\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8503 - acc: 0.8053 - val_loss: 0.9626 - val_acc: 0.7580\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8527 - acc: 0.8085 - val_loss: 0.9503 - val_acc: 0.7640\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8494 - acc: 0.8060 - val_loss: 0.9794 - val_acc: 0.7450\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8498 - acc: 0.8080 - val_loss: 0.9706 - val_acc: 0.7560\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8513 - acc: 0.8055 - val_loss: 0.9521 - val_acc: 0.7590\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8476 - acc: 0.8075 - val_loss: 0.9576 - val_acc: 0.7660\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8494 - acc: 0.8063 - val_loss: 0.9598 - val_acc: 0.7650\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8498 - acc: 0.8027 - val_loss: 0.9513 - val_acc: 0.7560\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8493 - acc: 0.8043 - val_loss: 0.9539 - val_acc: 0.7650\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8476 - acc: 0.8088 - val_loss: 0.9705 - val_acc: 0.7440\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8472 - acc: 0.8099 - val_loss: 0.9549 - val_acc: 0.7600\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8475 - acc: 0.8059 - val_loss: 0.9649 - val_acc: 0.7640\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8470 - acc: 0.8067 - val_loss: 0.9531 - val_acc: 0.7550\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8457 - acc: 0.8099 - val_loss: 1.0124 - val_acc: 0.7300\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8519 - acc: 0.8069 - val_loss: 0.9495 - val_acc: 0.7660\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8478 - acc: 0.8051 - val_loss: 0.9465 - val_acc: 0.7680\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8450 - acc: 0.8093 - val_loss: 0.9498 - val_acc: 0.7580\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8490 - acc: 0.8087 - val_loss: 0.9561 - val_acc: 0.7570\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8470 - acc: 0.8077 - val_loss: 0.9606 - val_acc: 0.7480\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8460 - acc: 0.8092 - val_loss: 0.9801 - val_acc: 0.7340\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8481 - acc: 0.8072 - val_loss: 0.9572 - val_acc: 0.7660\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8460 - acc: 0.8072 - val_loss: 0.9622 - val_acc: 0.7650\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8448 - acc: 0.8092 - val_loss: 0.9686 - val_acc: 0.7420\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8463 - acc: 0.8084 - val_loss: 0.9444 - val_acc: 0.7730\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8450 - acc: 0.8068 - val_loss: 0.9682 - val_acc: 0.7470\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8473 - acc: 0.8039 - val_loss: 0.9727 - val_acc: 0.7500\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8439 - acc: 0.8089 - val_loss: 0.9747 - val_acc: 0.7540\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8470 - acc: 0.8059 - val_loss: 0.9510 - val_acc: 0.7520\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8435 - acc: 0.8112 - val_loss: 0.9462 - val_acc: 0.7670\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8436 - acc: 0.8081 - val_loss: 0.9557 - val_acc: 0.7430\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8441 - acc: 0.8093 - val_loss: 0.9514 - val_acc: 0.7590\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8426 - acc: 0.8097 - val_loss: 0.9871 - val_acc: 0.7440\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8457 - acc: 0.8107 - val_loss: 0.9545 - val_acc: 0.7600\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8433 - acc: 0.8119 - val_loss: 0.9615 - val_acc: 0.7490\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8428 - acc: 0.8100 - val_loss: 0.9473 - val_acc: 0.7650\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8427 - acc: 0.8115 - val_loss: 0.9489 - val_acc: 0.7610\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8418 - acc: 0.8101 - val_loss: 0.9449 - val_acc: 0.7660\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8424 - acc: 0.8088 - val_loss: 0.9484 - val_acc: 0.7670\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8423 - acc: 0.8101 - val_loss: 0.9542 - val_acc: 0.7520\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8412 - acc: 0.8096 - val_loss: 0.9646 - val_acc: 0.7620\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8453 - acc: 0.8076 - val_loss: 1.0023 - val_acc: 0.7560\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8442 - acc: 0.8079 - val_loss: 0.9497 - val_acc: 0.7640\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8417 - acc: 0.8099 - val_loss: 0.9529 - val_acc: 0.7620\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8397 - acc: 0.8101 - val_loss: 0.9615 - val_acc: 0.7610\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8416 - acc: 0.8120 - val_loss: 0.9466 - val_acc: 0.7600\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8408 - acc: 0.8097 - val_loss: 0.9436 - val_acc: 0.7580\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8393 - acc: 0.8136 - val_loss: 0.9485 - val_acc: 0.7550\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8416 - acc: 0.8111 - val_loss: 0.9582 - val_acc: 0.7670\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8407 - acc: 0.8084 - val_loss: 0.9437 - val_acc: 0.7680\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8395 - acc: 0.8135 - val_loss: 0.9471 - val_acc: 0.7710\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8390 - acc: 0.8129 - val_loss: 0.9445 - val_acc: 0.7560\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8406 - acc: 0.8115 - val_loss: 0.9473 - val_acc: 0.7590\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8446 - acc: 0.8063 - val_loss: 0.9545 - val_acc: 0.7500\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8414 - acc: 0.8089 - val_loss: 0.9725 - val_acc: 0.7520\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8419 - acc: 0.8069 - val_loss: 0.9432 - val_acc: 0.7650\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8378 - acc: 0.8124 - val_loss: 0.9526 - val_acc: 0.7560\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8373 - acc: 0.8115 - val_loss: 0.9465 - val_acc: 0.7590\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8401 - acc: 0.8109 - val_loss: 0.9451 - val_acc: 0.7650\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8387 - acc: 0.8092 - val_loss: 0.9446 - val_acc: 0.7700\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8366 - acc: 0.8119 - val_loss: 0.9543 - val_acc: 0.7580\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8417 - acc: 0.8072 - val_loss: 0.9606 - val_acc: 0.7650\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8376 - acc: 0.8132 - val_loss: 0.9400 - val_acc: 0.7620\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8367 - acc: 0.8137 - val_loss: 0.9495 - val_acc: 0.7640\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8427 - acc: 0.8113 - val_loss: 0.9479 - val_acc: 0.7630\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8358 - acc: 0.8121 - val_loss: 0.9703 - val_acc: 0.7490\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8372 - acc: 0.8143 - val_loss: 0.9655 - val_acc: 0.7480\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8376 - acc: 0.8119 - val_loss: 0.9443 - val_acc: 0.7640\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8340 - acc: 0.8124 - val_loss: 0.9471 - val_acc: 0.7650\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8365 - acc: 0.8131 - val_loss: 0.9762 - val_acc: 0.7470\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8395 - acc: 0.8113 - val_loss: 0.9433 - val_acc: 0.7660\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8359 - acc: 0.8125 - val_loss: 0.9389 - val_acc: 0.7720\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8359 - acc: 0.8132 - val_loss: 0.9425 - val_acc: 0.7670\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8356 - acc: 0.8139 - val_loss: 0.9450 - val_acc: 0.7670\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8369 - acc: 0.8140 - val_loss: 0.9483 - val_acc: 0.7680\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8420 - acc: 0.8099 - val_loss: 0.9646 - val_acc: 0.7630\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8368 - acc: 0.8132 - val_loss: 0.9568 - val_acc: 0.7660\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8353 - acc: 0.8131 - val_loss: 0.9423 - val_acc: 0.7680\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8353 - acc: 0.8131 - val_loss: 0.9432 - val_acc: 0.7610\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8365 - acc: 0.8108 - val_loss: 0.9377 - val_acc: 0.7660\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8368 - acc: 0.812 - 0s 24us/step - loss: 0.8360 - acc: 0.8124 - val_loss: 0.9432 - val_acc: 0.7640\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8350 - acc: 0.8153 - val_loss: 0.9594 - val_acc: 0.7610\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8388 - acc: 0.8073 - val_loss: 0.9367 - val_acc: 0.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8334 - acc: 0.8161 - val_loss: 0.9499 - val_acc: 0.7540\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8365 - acc: 0.8093 - val_loss: 0.9500 - val_acc: 0.7590\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8393 - acc: 0.8093 - val_loss: 0.9502 - val_acc: 0.7670\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8362 - acc: 0.8117 - val_loss: 0.9400 - val_acc: 0.7650\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8387 - acc: 0.8107 - val_loss: 0.9523 - val_acc: 0.7640\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8343 - acc: 0.8119 - val_loss: 0.9460 - val_acc: 0.7650\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8305 - acc: 0.8164 - val_loss: 0.9513 - val_acc: 0.7630\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8341 - acc: 0.8127 - val_loss: 0.9561 - val_acc: 0.7640\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8334 - acc: 0.8167 - val_loss: 0.9351 - val_acc: 0.7700\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8332 - acc: 0.8131 - val_loss: 0.9414 - val_acc: 0.7640\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8310 - acc: 0.8179 - val_loss: 0.9707 - val_acc: 0.7490\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8375 - acc: 0.8136 - val_loss: 0.9483 - val_acc: 0.7560\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8333 - acc: 0.8145 - val_loss: 0.9351 - val_acc: 0.7660\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8308 - acc: 0.8172 - val_loss: 1.0324 - val_acc: 0.7440\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8396 - acc: 0.8099 - val_loss: 0.9435 - val_acc: 0.7660\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8348 - acc: 0.8139 - val_loss: 0.9529 - val_acc: 0.7640\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8325 - acc: 0.8145 - val_loss: 0.9479 - val_acc: 0.7570\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8330 - acc: 0.8120 - val_loss: 0.9495 - val_acc: 0.7620\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8315 - acc: 0.8161 - val_loss: 0.9421 - val_acc: 0.7630\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8304 - acc: 0.8173 - val_loss: 0.9394 - val_acc: 0.7670\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8342 - acc: 0.8129 - val_loss: 0.9473 - val_acc: 0.7650\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8299 - acc: 0.8157 - val_loss: 1.0788 - val_acc: 0.7330\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8344 - acc: 0.8143 - val_loss: 0.9356 - val_acc: 0.7700\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8351 - acc: 0.8119 - val_loss: 0.9501 - val_acc: 0.7630\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8343 - acc: 0.8119 - val_loss: 0.9547 - val_acc: 0.7460\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8297 - acc: 0.8160 - val_loss: 0.9529 - val_acc: 0.7470\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8317 - acc: 0.8149 - val_loss: 0.9528 - val_acc: 0.7630\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8330 - acc: 0.8131 - val_loss: 0.9570 - val_acc: 0.7670\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8305 - acc: 0.8151 - val_loss: 0.9432 - val_acc: 0.7600\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8306 - acc: 0.8163 - val_loss: 0.9593 - val_acc: 0.7690\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8317 - acc: 0.8165 - val_loss: 0.9591 - val_acc: 0.7560\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8320 - acc: 0.8131 - val_loss: 0.9666 - val_acc: 0.7580\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8411 - acc: 0.8095 - val_loss: 0.9399 - val_acc: 0.7650\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8273 - acc: 0.8171 - val_loss: 0.9575 - val_acc: 0.7630\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8279 - acc: 0.8180 - val_loss: 0.9509 - val_acc: 0.7620\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8290 - acc: 0.8181 - val_loss: 0.9619 - val_acc: 0.7570\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8290 - acc: 0.8181 - val_loss: 0.9354 - val_acc: 0.7660\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8260 - acc: 0.8177 - val_loss: 0.9354 - val_acc: 0.7670\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8260 - acc: 0.8183 - val_loss: 0.9425 - val_acc: 0.7640\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8261 - acc: 0.8197 - val_loss: 0.9394 - val_acc: 0.7650\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8250 - acc: 0.8168 - val_loss: 1.0025 - val_acc: 0.7190\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8317 - acc: 0.8120 - val_loss: 0.9413 - val_acc: 0.7620\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8288 - acc: 0.8156 - val_loss: 0.9346 - val_acc: 0.7690\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8294 - acc: 0.8165 - val_loss: 0.9646 - val_acc: 0.7650\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8284 - acc: 0.8153 - val_loss: 0.9465 - val_acc: 0.7590\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8271 - acc: 0.8168 - val_loss: 0.9874 - val_acc: 0.7510\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8311 - acc: 0.8161 - val_loss: 0.9386 - val_acc: 0.7660\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8280 - acc: 0.8135 - val_loss: 0.9434 - val_acc: 0.7610\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8309 - acc: 0.8133 - val_loss: 0.9397 - val_acc: 0.7730\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8260 - acc: 0.8176 - val_loss: 0.9429 - val_acc: 0.7670\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8273 - acc: 0.8188 - val_loss: 0.9357 - val_acc: 0.7710\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8268 - acc: 0.8175 - val_loss: 0.9419 - val_acc: 0.7620\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8326 - acc: 0.8108 - val_loss: 0.9491 - val_acc: 0.7600\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8271 - acc: 0.8163 - val_loss: 0.9393 - val_acc: 0.7630\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8242 - acc: 0.8205 - val_loss: 0.9442 - val_acc: 0.7660\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8246 - acc: 0.8189 - val_loss: 0.9293 - val_acc: 0.7670\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8259 - acc: 0.8181 - val_loss: 0.9433 - val_acc: 0.7730\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8244 - acc: 0.8205 - val_loss: 0.9489 - val_acc: 0.7590\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8286 - acc: 0.8141 - val_loss: 0.9661 - val_acc: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8275 - acc: 0.8157 - val_loss: 0.9706 - val_acc: 0.7690\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8247 - acc: 0.8180 - val_loss: 0.9606 - val_acc: 0.7580\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8286 - acc: 0.8128 - val_loss: 0.9297 - val_acc: 0.7720\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8322 - acc: 0.8113 - val_loss: 0.9295 - val_acc: 0.7740\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8236 - acc: 0.8211 - val_loss: 0.9381 - val_acc: 0.7670\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8248 - acc: 0.8192 - val_loss: 0.9332 - val_acc: 0.7700\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8244 - acc: 0.8187 - val_loss: 0.9304 - val_acc: 0.7710\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8244 - acc: 0.8201 - val_loss: 0.9420 - val_acc: 0.7700\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8245 - acc: 0.8181 - val_loss: 0.9472 - val_acc: 0.7670\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8248 - acc: 0.8141 - val_loss: 0.9567 - val_acc: 0.7610\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8228 - acc: 0.8168 - val_loss: 0.9546 - val_acc: 0.7650\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8221 - acc: 0.8232 - val_loss: 0.9812 - val_acc: 0.7550\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8252 - acc: 0.8168 - val_loss: 0.9314 - val_acc: 0.7710\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8320 - acc: 0.8100 - val_loss: 0.9388 - val_acc: 0.7650\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8202 - acc: 0.819 - 0s 24us/step - loss: 0.8215 - acc: 0.8185 - val_loss: 0.9380 - val_acc: 0.7620\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8320 - acc: 0.8125 - val_loss: 0.9300 - val_acc: 0.7710\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8254 - acc: 0.8159 - val_loss: 0.9395 - val_acc: 0.7700\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8210 - acc: 0.8225 - val_loss: 0.9621 - val_acc: 0.7620\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8259 - acc: 0.8161 - val_loss: 0.9311 - val_acc: 0.7760\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8215 - acc: 0.8213 - val_loss: 0.9328 - val_acc: 0.7650\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8219 - acc: 0.8185 - val_loss: 0.9320 - val_acc: 0.7670\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8191 - acc: 0.8225 - val_loss: 0.9407 - val_acc: 0.7630\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8206 - acc: 0.8180 - val_loss: 0.9743 - val_acc: 0.7560\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8212 - acc: 0.8157 - val_loss: 0.9483 - val_acc: 0.7710\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8265 - acc: 0.8147 - val_loss: 0.9340 - val_acc: 0.7660\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8199 - acc: 0.8207 - val_loss: 0.9508 - val_acc: 0.7610\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8234 - acc: 0.8173 - val_loss: 0.9427 - val_acc: 0.7710\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8278 - acc: 0.8149 - val_loss: 0.9557 - val_acc: 0.7660\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8221 - acc: 0.8209 - val_loss: 0.9398 - val_acc: 0.7660\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8215 - acc: 0.8196 - val_loss: 0.9370 - val_acc: 0.7700\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8193 - acc: 0.8233 - val_loss: 0.9290 - val_acc: 0.7680\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8205 - acc: 0.8195 - val_loss: 0.9304 - val_acc: 0.7710\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8264 - acc: 0.8187 - val_loss: 0.9271 - val_acc: 0.7720\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8172 - acc: 0.8220 - val_loss: 0.9659 - val_acc: 0.7560\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8290 - acc: 0.8155 - val_loss: 0.9498 - val_acc: 0.7630\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8201 - acc: 0.8193 - val_loss: 0.9322 - val_acc: 0.7660\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8229 - acc: 0.8161 - val_loss: 0.9426 - val_acc: 0.7700\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8183 - acc: 0.8221 - val_loss: 0.9548 - val_acc: 0.7660\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8186 - acc: 0.8195 - val_loss: 0.9439 - val_acc: 0.7550\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8202 - acc: 0.8223 - val_loss: 0.9372 - val_acc: 0.7630\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8227 - acc: 0.8155 - val_loss: 0.9424 - val_acc: 0.7650\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8198 - acc: 0.8197 - val_loss: 0.9361 - val_acc: 0.7670\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8201 - acc: 0.8180 - val_loss: 0.9327 - val_acc: 0.7640\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8172 - acc: 0.8215 - val_loss: 0.9493 - val_acc: 0.7690\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8237 - acc: 0.8169 - val_loss: 0.9275 - val_acc: 0.7750\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8160 - acc: 0.8225 - val_loss: 0.9401 - val_acc: 0.7690\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8191 - acc: 0.8197 - val_loss: 0.9341 - val_acc: 0.7680\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8270 - acc: 0.8125 - val_loss: 0.9374 - val_acc: 0.7650\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8174 - acc: 0.8224 - val_loss: 0.9393 - val_acc: 0.7660\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8208 - acc: 0.8167 - val_loss: 0.9675 - val_acc: 0.7630\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8198 - acc: 0.8173 - val_loss: 0.9306 - val_acc: 0.7700\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8162 - acc: 0.8213 - val_loss: 0.9577 - val_acc: 0.7530\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8182 - acc: 0.8203 - val_loss: 0.9800 - val_acc: 0.7340\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8182 - acc: 0.8211 - val_loss: 0.9402 - val_acc: 0.7730\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8155 - acc: 0.8213 - val_loss: 0.9304 - val_acc: 0.7720\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8151 - acc: 0.8216 - val_loss: 0.9246 - val_acc: 0.7760\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8149 - acc: 0.8217 - val_loss: 0.9906 - val_acc: 0.7330\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8264 - acc: 0.8157 - val_loss: 0.9397 - val_acc: 0.7530\n",
      "Epoch 944/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8200 - acc: 0.8177 - val_loss: 0.9766 - val_acc: 0.7630\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8191 - acc: 0.8201 - val_loss: 0.9375 - val_acc: 0.7710\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8227 - acc: 0.8183 - val_loss: 0.9498 - val_acc: 0.7690\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8157 - acc: 0.8248 - val_loss: 0.9319 - val_acc: 0.7670\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8168 - acc: 0.8205 - val_loss: 0.9800 - val_acc: 0.7250\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8211 - acc: 0.8196 - val_loss: 1.0243 - val_acc: 0.7520\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8249 - acc: 0.8137 - val_loss: 0.9285 - val_acc: 0.7690\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8196 - acc: 0.8195 - val_loss: 1.0045 - val_acc: 0.7470\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8219 - acc: 0.8163 - val_loss: 0.9778 - val_acc: 0.7510\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8233 - acc: 0.8149 - val_loss: 0.9590 - val_acc: 0.7440\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8191 - acc: 0.8204 - val_loss: 0.9518 - val_acc: 0.7650\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8144 - acc: 0.8209 - val_loss: 0.9348 - val_acc: 0.7640\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8156 - acc: 0.8235 - val_loss: 0.9371 - val_acc: 0.7700\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8135 - acc: 0.8239 - val_loss: 0.9387 - val_acc: 0.7540\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8150 - acc: 0.8192 - val_loss: 0.9381 - val_acc: 0.7750\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8127 - acc: 0.8199 - val_loss: 0.9334 - val_acc: 0.7710\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8141 - acc: 0.8231 - val_loss: 1.0025 - val_acc: 0.7160\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8326 - acc: 0.8096 - val_loss: 0.9356 - val_acc: 0.7660\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8169 - acc: 0.8203 - val_loss: 0.9405 - val_acc: 0.7640\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8170 - acc: 0.8216 - val_loss: 0.9608 - val_acc: 0.7670\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8190 - acc: 0.8211 - val_loss: 0.9363 - val_acc: 0.7690\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8146 - acc: 0.8209 - val_loss: 0.9588 - val_acc: 0.7580\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8135 - acc: 0.8236 - val_loss: 0.9291 - val_acc: 0.7740\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8176 - acc: 0.8171 - val_loss: 0.9241 - val_acc: 0.7760\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8308 - acc: 0.8137 - val_loss: 0.9367 - val_acc: 0.7660\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8151 - acc: 0.8212 - val_loss: 0.9779 - val_acc: 0.7400\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8158 - acc: 0.8189 - val_loss: 0.9352 - val_acc: 0.7710\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8120 - acc: 0.8227 - val_loss: 0.9329 - val_acc: 0.7780\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8237 - acc: 0.8145 - val_loss: 0.9293 - val_acc: 0.7670\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8195 - acc: 0.8145 - val_loss: 0.9384 - val_acc: 0.7640\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8201 - acc: 0.8187 - val_loss: 0.9471 - val_acc: 0.7650\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8136 - acc: 0.8212 - val_loss: 0.9673 - val_acc: 0.7640\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8144 - acc: 0.8201 - val_loss: 0.9387 - val_acc: 0.7730\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8308 - acc: 0.8123 - val_loss: 0.9369 - val_acc: 0.7710\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8149 - acc: 0.8217 - val_loss: 0.9343 - val_acc: 0.7660\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8105 - acc: 0.8224 - val_loss: 0.9412 - val_acc: 0.7650\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8092 - acc: 0.8221 - val_loss: 0.9309 - val_acc: 0.7690\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8095 - acc: 0.8241 - val_loss: 0.9309 - val_acc: 0.7630\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8100 - acc: 0.8233 - val_loss: 1.0672 - val_acc: 0.7450\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8156 - acc: 0.8233 - val_loss: 0.9557 - val_acc: 0.7480\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8403 - acc: 0.8071 - val_loss: 0.9621 - val_acc: 0.7660\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8108 - acc: 0.8231 - val_loss: 0.9444 - val_acc: 0.7710\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8229 - acc: 0.8125 - val_loss: 0.9346 - val_acc: 0.7710\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8107 - acc: 0.8197 - val_loss: 0.9446 - val_acc: 0.7730\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8148 - acc: 0.8228 - val_loss: 0.9798 - val_acc: 0.7550\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8147 - acc: 0.8208 - val_loss: 0.9550 - val_acc: 0.7670\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8153 - acc: 0.8197 - val_loss: 0.9231 - val_acc: 0.7740\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8140 - acc: 0.8224 - val_loss: 0.9469 - val_acc: 0.7590\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8270 - acc: 0.8112 - val_loss: 0.9541 - val_acc: 0.7600\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8080 - acc: 0.8241 - val_loss: 0.9419 - val_acc: 0.7700\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8145 - acc: 0.8169 - val_loss: 0.9367 - val_acc: 0.7640\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8195 - acc: 0.8168 - val_loss: 0.9288 - val_acc: 0.7650\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8086 - acc: 0.8252 - val_loss: 0.9382 - val_acc: 0.7510\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8170 - acc: 0.8211 - val_loss: 0.9260 - val_acc: 0.7700\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8183 - acc: 0.8205 - val_loss: 0.9355 - val_acc: 0.7620\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8137 - acc: 0.8185 - val_loss: 0.9468 - val_acc: 0.7500\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8136 - acc: 0.8197 - val_loss: 0.9815 - val_acc: 0.7500\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 20us/step\n",
      "1500/1500 [==============================] - 0s 20us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8469060552597046, 0.7910666666666667]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9994435156186422, 0.745333333492279]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0703 14:40:30.426020  2256 deprecation.py:506] From C:\\Users\\kylej\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.9678 - acc: 0.1539 - val_loss: 1.9237 - val_acc: 0.1710\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9466 - acc: 0.1576 - val_loss: 1.9085 - val_acc: 0.1940\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9306 - acc: 0.1785 - val_loss: 1.8953 - val_acc: 0.2160\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9130 - acc: 0.1905 - val_loss: 1.8810 - val_acc: 0.2250\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9066 - acc: 0.1945 - val_loss: 1.8657 - val_acc: 0.2490\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8901 - acc: 0.2063 - val_loss: 1.8502 - val_acc: 0.2610\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8717 - acc: 0.2249 - val_loss: 1.8310 - val_acc: 0.2840\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8595 - acc: 0.2196 - val_loss: 1.8111 - val_acc: 0.2970\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8389 - acc: 0.2400 - val_loss: 1.7891 - val_acc: 0.3020\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8304 - acc: 0.2435 - val_loss: 1.7660 - val_acc: 0.3210\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.8064 - acc: 0.2568 - val_loss: 1.7405 - val_acc: 0.3370\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.7950 - acc: 0.2628 - val_loss: 1.7157 - val_acc: 0.3490\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7722 - acc: 0.2743 - val_loss: 1.6875 - val_acc: 0.3650\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7426 - acc: 0.2888 - val_loss: 1.6599 - val_acc: 0.3750\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7230 - acc: 0.2979 - val_loss: 1.6295 - val_acc: 0.3970\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7198 - acc: 0.3029 - val_loss: 1.6027 - val_acc: 0.4030\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6844 - acc: 0.3149 - val_loss: 1.5747 - val_acc: 0.4200\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6680 - acc: 0.3269 - val_loss: 1.5498 - val_acc: 0.4270\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6507 - acc: 0.3309 - val_loss: 1.5231 - val_acc: 0.4560\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.6293 - acc: 0.3449 - val_loss: 1.5001 - val_acc: 0.4650\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6078 - acc: 0.3515 - val_loss: 1.4754 - val_acc: 0.4810\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5986 - acc: 0.3563 - val_loss: 1.4540 - val_acc: 0.4940\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5819 - acc: 0.3541 - val_loss: 1.4313 - val_acc: 0.4940\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5579 - acc: 0.3771 - val_loss: 1.4092 - val_acc: 0.5090\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5471 - acc: 0.3747 - val_loss: 1.3863 - val_acc: 0.5170\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5280 - acc: 0.3817 - val_loss: 1.3638 - val_acc: 0.5260\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5138 - acc: 0.3895 - val_loss: 1.3463 - val_acc: 0.5470\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4911 - acc: 0.4064 - val_loss: 1.3260 - val_acc: 0.5630\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4755 - acc: 0.4100 - val_loss: 1.3044 - val_acc: 0.5660\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4680 - acc: 0.4144 - val_loss: 1.2825 - val_acc: 0.5780\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.4519 - acc: 0.4175 - val_loss: 1.2664 - val_acc: 0.5860\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4368 - acc: 0.4256 - val_loss: 1.2481 - val_acc: 0.6000\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4165 - acc: 0.4385 - val_loss: 1.2313 - val_acc: 0.6060\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4164 - acc: 0.4428 - val_loss: 1.2140 - val_acc: 0.6150\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.4039 - acc: 0.4440 - val_loss: 1.1981 - val_acc: 0.6220\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3861 - acc: 0.4581 - val_loss: 1.1817 - val_acc: 0.6310\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3695 - acc: 0.4637 - val_loss: 1.1646 - val_acc: 0.6350\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3724 - acc: 0.4580 - val_loss: 1.1509 - val_acc: 0.6450\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3501 - acc: 0.4724 - val_loss: 1.1364 - val_acc: 0.6420\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3338 - acc: 0.4839 - val_loss: 1.1214 - val_acc: 0.6510\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3293 - acc: 0.4836 - val_loss: 1.1092 - val_acc: 0.6590\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3073 - acc: 0.4936 - val_loss: 1.0952 - val_acc: 0.6600\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3047 - acc: 0.4947 - val_loss: 1.0810 - val_acc: 0.6590\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3054 - acc: 0.4983 - val_loss: 1.0720 - val_acc: 0.6650\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2906 - acc: 0.5025 - val_loss: 1.0618 - val_acc: 0.6640\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2867 - acc: 0.5021 - val_loss: 1.0467 - val_acc: 0.6630\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2549 - acc: 0.5077 - val_loss: 1.0340 - val_acc: 0.6690\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2648 - acc: 0.5039 - val_loss: 1.0240 - val_acc: 0.6750\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2497 - acc: 0.5203 - val_loss: 1.0137 - val_acc: 0.6780\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2351 - acc: 0.5177 - val_loss: 1.0032 - val_acc: 0.6810\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2315 - acc: 0.5237 - val_loss: 0.9941 - val_acc: 0.6840\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2288 - acc: 0.5279 - val_loss: 0.9822 - val_acc: 0.6860\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2006 - acc: 0.5401 - val_loss: 0.9717 - val_acc: 0.6890\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1879 - acc: 0.5387 - val_loss: 0.9608 - val_acc: 0.6960\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1842 - acc: 0.5443 - val_loss: 0.9542 - val_acc: 0.7000\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1949 - acc: 0.5383 - val_loss: 0.9477 - val_acc: 0.7030\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1735 - acc: 0.5509 - val_loss: 0.9376 - val_acc: 0.7010\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1763 - acc: 0.5493 - val_loss: 0.9309 - val_acc: 0.7020\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1603 - acc: 0.5507 - val_loss: 0.9210 - val_acc: 0.6990\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1592 - acc: 0.5601 - val_loss: 0.9141 - val_acc: 0.7030\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1599 - acc: 0.5612 - val_loss: 0.9076 - val_acc: 0.7080\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1463 - acc: 0.5693 - val_loss: 0.8981 - val_acc: 0.7060\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1409 - acc: 0.5723 - val_loss: 0.8916 - val_acc: 0.7130\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1290 - acc: 0.5656 - val_loss: 0.8867 - val_acc: 0.7120\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.1301 - acc: 0.5700 - val_loss: 0.8788 - val_acc: 0.7170\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1151 - acc: 0.5723 - val_loss: 0.8727 - val_acc: 0.7140\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0918 - acc: 0.5889 - val_loss: 0.8653 - val_acc: 0.7160\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1036 - acc: 0.5781 - val_loss: 0.8608 - val_acc: 0.7220\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0970 - acc: 0.5871 - val_loss: 0.8549 - val_acc: 0.7210\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.0948 - acc: 0.5917 - val_loss: 0.8507 - val_acc: 0.7180\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0762 - acc: 0.5896 - val_loss: 0.8429 - val_acc: 0.7190\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0871 - acc: 0.5909 - val_loss: 0.8402 - val_acc: 0.7280\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0793 - acc: 0.5949 - val_loss: 0.8354 - val_acc: 0.7260\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0740 - acc: 0.6000 - val_loss: 0.8291 - val_acc: 0.7360\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0585 - acc: 0.6089 - val_loss: 0.8247 - val_acc: 0.7280\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0596 - acc: 0.6001 - val_loss: 0.8203 - val_acc: 0.7370\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0478 - acc: 0.6041 - val_loss: 0.8152 - val_acc: 0.7350\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0620 - acc: 0.6089 - val_loss: 0.8119 - val_acc: 0.7380\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0364 - acc: 0.6071 - val_loss: 0.8041 - val_acc: 0.7400\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0376 - acc: 0.6121 - val_loss: 0.7996 - val_acc: 0.7380\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0257 - acc: 0.6144 - val_loss: 0.7962 - val_acc: 0.7390\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0326 - acc: 0.6113 - val_loss: 0.7928 - val_acc: 0.7380\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0243 - acc: 0.6163 - val_loss: 0.7886 - val_acc: 0.7400\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0139 - acc: 0.6251 - val_loss: 0.7845 - val_acc: 0.7400\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0117 - acc: 0.6203 - val_loss: 0.7797 - val_acc: 0.7410\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0120 - acc: 0.6239 - val_loss: 0.7768 - val_acc: 0.7440\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0150 - acc: 0.6171 - val_loss: 0.7753 - val_acc: 0.7480\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.0083 - acc: 0.6168 - val_loss: 0.7728 - val_acc: 0.7440\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9976 - acc: 0.6361 - val_loss: 0.7691 - val_acc: 0.7470\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9863 - acc: 0.6288 - val_loss: 0.7630 - val_acc: 0.7440\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9808 - acc: 0.6348 - val_loss: 0.7574 - val_acc: 0.7460\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9772 - acc: 0.6341 - val_loss: 0.7541 - val_acc: 0.7470\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9781 - acc: 0.6297 - val_loss: 0.7534 - val_acc: 0.7470\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9775 - acc: 0.6385 - val_loss: 0.7493 - val_acc: 0.7520\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9666 - acc: 0.6353 - val_loss: 0.7468 - val_acc: 0.7490\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9660 - acc: 0.6373 - val_loss: 0.7441 - val_acc: 0.7500\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9784 - acc: 0.6349 - val_loss: 0.7411 - val_acc: 0.7500\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9685 - acc: 0.6320 - val_loss: 0.7384 - val_acc: 0.7490\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9605 - acc: 0.6401 - val_loss: 0.7338 - val_acc: 0.7550\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9438 - acc: 0.6461 - val_loss: 0.7320 - val_acc: 0.7480\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9488 - acc: 0.6476 - val_loss: 0.7324 - val_acc: 0.7540\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9572 - acc: 0.6377 - val_loss: 0.7289 - val_acc: 0.7540\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9331 - acc: 0.6603 - val_loss: 0.7244 - val_acc: 0.7570\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9368 - acc: 0.6488 - val_loss: 0.7219 - val_acc: 0.7540\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9353 - acc: 0.6469 - val_loss: 0.7198 - val_acc: 0.7560\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9301 - acc: 0.6480 - val_loss: 0.7150 - val_acc: 0.7540\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9383 - acc: 0.6499 - val_loss: 0.7159 - val_acc: 0.7590\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9302 - acc: 0.6496 - val_loss: 0.7172 - val_acc: 0.7610\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9217 - acc: 0.6605 - val_loss: 0.7120 - val_acc: 0.7580\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9180 - acc: 0.6533 - val_loss: 0.7074 - val_acc: 0.7590\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9113 - acc: 0.6633 - val_loss: 0.7034 - val_acc: 0.7590\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9024 - acc: 0.6649 - val_loss: 0.7025 - val_acc: 0.7600\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9003 - acc: 0.6613 - val_loss: 0.7003 - val_acc: 0.7590\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9003 - acc: 0.6707 - val_loss: 0.6974 - val_acc: 0.7610\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8833 - acc: 0.6716 - val_loss: 0.6959 - val_acc: 0.7600\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8991 - acc: 0.6649 - val_loss: 0.6931 - val_acc: 0.7620\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8904 - acc: 0.6661 - val_loss: 0.6923 - val_acc: 0.7630\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9010 - acc: 0.6663 - val_loss: 0.6906 - val_acc: 0.7610\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9031 - acc: 0.6636 - val_loss: 0.6905 - val_acc: 0.7620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8835 - acc: 0.6753 - val_loss: 0.6856 - val_acc: 0.7600\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8803 - acc: 0.6737 - val_loss: 0.6828 - val_acc: 0.7620\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8735 - acc: 0.6809 - val_loss: 0.6820 - val_acc: 0.7640\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8823 - acc: 0.6664 - val_loss: 0.6795 - val_acc: 0.7670\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8786 - acc: 0.6693 - val_loss: 0.6801 - val_acc: 0.7640\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8596 - acc: 0.6825 - val_loss: 0.6783 - val_acc: 0.7670\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8745 - acc: 0.6785 - val_loss: 0.6757 - val_acc: 0.7670\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8712 - acc: 0.6695 - val_loss: 0.6747 - val_acc: 0.7660\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8693 - acc: 0.6733 - val_loss: 0.6739 - val_acc: 0.7650\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8482 - acc: 0.6799 - val_loss: 0.6686 - val_acc: 0.7640\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8598 - acc: 0.6769 - val_loss: 0.6696 - val_acc: 0.7640\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8620 - acc: 0.6767 - val_loss: 0.6711 - val_acc: 0.7680\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8585 - acc: 0.6808 - val_loss: 0.6664 - val_acc: 0.7660\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8493 - acc: 0.6796 - val_loss: 0.6642 - val_acc: 0.7660\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8588 - acc: 0.6803 - val_loss: 0.6626 - val_acc: 0.7680\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8548 - acc: 0.6843 - val_loss: 0.6648 - val_acc: 0.7680\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8456 - acc: 0.6857 - val_loss: 0.6634 - val_acc: 0.7680\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8409 - acc: 0.6896 - val_loss: 0.6595 - val_acc: 0.7620\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8453 - acc: 0.6832 - val_loss: 0.6592 - val_acc: 0.7660\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8441 - acc: 0.6879 - val_loss: 0.6585 - val_acc: 0.7690\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8390 - acc: 0.6835 - val_loss: 0.6574 - val_acc: 0.7700\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8513 - acc: 0.6808 - val_loss: 0.6570 - val_acc: 0.7680\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8360 - acc: 0.6904 - val_loss: 0.6551 - val_acc: 0.7670\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8286 - acc: 0.6924 - val_loss: 0.6525 - val_acc: 0.7670\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8330 - acc: 0.6901 - val_loss: 0.6517 - val_acc: 0.7680\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8356 - acc: 0.6876 - val_loss: 0.6514 - val_acc: 0.7700\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8443 - acc: 0.6849 - val_loss: 0.6514 - val_acc: 0.7720\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8430 - acc: 0.6849 - val_loss: 0.6487 - val_acc: 0.7720\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8221 - acc: 0.6899 - val_loss: 0.6463 - val_acc: 0.7710\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8283 - acc: 0.6867 - val_loss: 0.6452 - val_acc: 0.7670\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8349 - acc: 0.6880 - val_loss: 0.6467 - val_acc: 0.7690\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8209 - acc: 0.6931 - val_loss: 0.6448 - val_acc: 0.7720\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8117 - acc: 0.7043 - val_loss: 0.6428 - val_acc: 0.7730\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7955 - acc: 0.6991 - val_loss: 0.6403 - val_acc: 0.7710\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8137 - acc: 0.6944 - val_loss: 0.6396 - val_acc: 0.7730\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8087 - acc: 0.7005 - val_loss: 0.6394 - val_acc: 0.7690\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7996 - acc: 0.7068 - val_loss: 0.6398 - val_acc: 0.7780\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8059 - acc: 0.6977 - val_loss: 0.6387 - val_acc: 0.7760\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8082 - acc: 0.6992 - val_loss: 0.6376 - val_acc: 0.7750\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8088 - acc: 0.7016 - val_loss: 0.6356 - val_acc: 0.7770\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8058 - acc: 0.6997 - val_loss: 0.6347 - val_acc: 0.7750\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8047 - acc: 0.6972 - val_loss: 0.6361 - val_acc: 0.7730\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7794 - acc: 0.7041 - val_loss: 0.6329 - val_acc: 0.7760\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7955 - acc: 0.6975 - val_loss: 0.6319 - val_acc: 0.7740\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7955 - acc: 0.7023 - val_loss: 0.6314 - val_acc: 0.7730\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8061 - acc: 0.6992 - val_loss: 0.6349 - val_acc: 0.7730\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7816 - acc: 0.7064 - val_loss: 0.6303 - val_acc: 0.7770\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7764 - acc: 0.7067 - val_loss: 0.6272 - val_acc: 0.7800\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7867 - acc: 0.7037 - val_loss: 0.6264 - val_acc: 0.7780\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7860 - acc: 0.7037 - val_loss: 0.6263 - val_acc: 0.7770\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7800 - acc: 0.7087 - val_loss: 0.6268 - val_acc: 0.7740\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7662 - acc: 0.7121 - val_loss: 0.6228 - val_acc: 0.7740\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7763 - acc: 0.7024 - val_loss: 0.6219 - val_acc: 0.7730\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7820 - acc: 0.7064 - val_loss: 0.6221 - val_acc: 0.7770\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7736 - acc: 0.7103 - val_loss: 0.6242 - val_acc: 0.7770\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7528 - acc: 0.7140 - val_loss: 0.6214 - val_acc: 0.7770\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7663 - acc: 0.7151 - val_loss: 0.6196 - val_acc: 0.7800\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7766 - acc: 0.7023 - val_loss: 0.6187 - val_acc: 0.7780\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7605 - acc: 0.7137 - val_loss: 0.6195 - val_acc: 0.7780\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7723 - acc: 0.7115 - val_loss: 0.6187 - val_acc: 0.7790\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7568 - acc: 0.7120 - val_loss: 0.6193 - val_acc: 0.7760\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7538 - acc: 0.7135 - val_loss: 0.6157 - val_acc: 0.7800\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7521 - acc: 0.7139 - val_loss: 0.6155 - val_acc: 0.7790\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7670 - acc: 0.7145 - val_loss: 0.6149 - val_acc: 0.7810\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7566 - acc: 0.7159 - val_loss: 0.6150 - val_acc: 0.7790\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7417 - acc: 0.7247 - val_loss: 0.6150 - val_acc: 0.7760\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7676 - acc: 0.7148 - val_loss: 0.6141 - val_acc: 0.7800\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7633 - acc: 0.7157 - val_loss: 0.6128 - val_acc: 0.7800\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7624 - acc: 0.7092 - val_loss: 0.6137 - val_acc: 0.7800\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7581 - acc: 0.7117 - val_loss: 0.6131 - val_acc: 0.7820\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7463 - acc: 0.7192 - val_loss: 0.6119 - val_acc: 0.7780\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7498 - acc: 0.7196 - val_loss: 0.6099 - val_acc: 0.7830\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7460 - acc: 0.7184 - val_loss: 0.6093 - val_acc: 0.7830\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7644 - acc: 0.7132 - val_loss: 0.6099 - val_acc: 0.7850\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7503 - acc: 0.7189 - val_loss: 0.6101 - val_acc: 0.7870\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7384 - acc: 0.7200 - val_loss: 0.6095 - val_acc: 0.7850\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7361 - acc: 0.7207 - val_loss: 0.6068 - val_acc: 0.7820\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7403 - acc: 0.7220 - val_loss: 0.6061 - val_acc: 0.7820\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7372 - acc: 0.7197 - val_loss: 0.6055 - val_acc: 0.7840\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7495 - acc: 0.7165 - val_loss: 0.6057 - val_acc: 0.7810\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7304 - acc: 0.7223 - val_loss: 0.6047 - val_acc: 0.7830\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step\n",
      "1500/1500 [==============================] - 0s 29us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45776414885520933, 0.8344]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6157087717056274, 0.7753333338101704]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 1.9271 - acc: 0.1765 - val_loss: 1.8939 - val_acc: 0.2417\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 1.8451 - acc: 0.2728 - val_loss: 1.7846 - val_acc: 0.3250\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 1.7036 - acc: 0.3735 - val_loss: 1.6067 - val_acc: 0.4287\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 1.5064 - acc: 0.4970 - val_loss: 1.3951 - val_acc: 0.5400\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 1.2978 - acc: 0.5957 - val_loss: 1.1986 - val_acc: 0.6220\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 1.1174 - acc: 0.6542 - val_loss: 1.0441 - val_acc: 0.6663\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.9828 - acc: 0.6884 - val_loss: 0.9363 - val_acc: 0.6923\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.8858 - acc: 0.7109 - val_loss: 0.8592 - val_acc: 0.7087\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.8157 - acc: 0.7273 - val_loss: 0.8025 - val_acc: 0.7220\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.7635 - acc: 0.7380 - val_loss: 0.7630 - val_acc: 0.7273\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.7234 - acc: 0.7471 - val_loss: 0.7319 - val_acc: 0.7360\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.6924 - acc: 0.7541 - val_loss: 0.7066 - val_acc: 0.7373\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.6671 - acc: 0.7604 - val_loss: 0.6895 - val_acc: 0.7450\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.6460 - acc: 0.7670 - val_loss: 0.6726 - val_acc: 0.7473\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.6285 - acc: 0.7727 - val_loss: 0.6605 - val_acc: 0.7500\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.6134 - acc: 0.7793 - val_loss: 0.6491 - val_acc: 0.7557\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.6000 - acc: 0.7822 - val_loss: 0.6407 - val_acc: 0.7603\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.5882 - acc: 0.7857 - val_loss: 0.6326 - val_acc: 0.7593\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.5780 - acc: 0.7898 - val_loss: 0.6276 - val_acc: 0.7650\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5685 - acc: 0.7929 - val_loss: 0.6193 - val_acc: 0.7657\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5596 - acc: 0.7976 - val_loss: 0.6139 - val_acc: 0.7680\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.5515 - acc: 0.8005 - val_loss: 0.6126 - val_acc: 0.7667\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5441 - acc: 0.8038 - val_loss: 0.6053 - val_acc: 0.7693\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5371 - acc: 0.8056 - val_loss: 0.6009 - val_acc: 0.7710\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5303 - acc: 0.8095 - val_loss: 0.5979 - val_acc: 0.7707\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5243 - acc: 0.8110 - val_loss: 0.5929 - val_acc: 0.7757\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5183 - acc: 0.8141 - val_loss: 0.5905 - val_acc: 0.7760\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5129 - acc: 0.8154 - val_loss: 0.5877 - val_acc: 0.7790\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5075 - acc: 0.8175 - val_loss: 0.5842 - val_acc: 0.7817\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.5023 - acc: 0.8198 - val_loss: 0.5826 - val_acc: 0.7843\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4974 - acc: 0.8214 - val_loss: 0.5790 - val_acc: 0.7837\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4929 - acc: 0.8234 - val_loss: 0.5762 - val_acc: 0.7843\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4882 - acc: 0.8255 - val_loss: 0.5751 - val_acc: 0.7857\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.4838 - acc: 0.8263 - val_loss: 0.5737 - val_acc: 0.7823\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4796 - acc: 0.8277 - val_loss: 0.5703 - val_acc: 0.7877\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4754 - acc: 0.8303 - val_loss: 0.5699 - val_acc: 0.7857\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4720 - acc: 0.8318 - val_loss: 0.5667 - val_acc: 0.7890\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4678 - acc: 0.8328 - val_loss: 0.5649 - val_acc: 0.7880\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4639 - acc: 0.8334 - val_loss: 0.5649 - val_acc: 0.7893\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4607 - acc: 0.8354 - val_loss: 0.5649 - val_acc: 0.7913\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4568 - acc: 0.8362 - val_loss: 0.5610 - val_acc: 0.7940\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4532 - acc: 0.8382 - val_loss: 0.5613 - val_acc: 0.7933\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4501 - acc: 0.8381 - val_loss: 0.5588 - val_acc: 0.7937\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4470 - acc: 0.8408 - val_loss: 0.5581 - val_acc: 0.7930\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4439 - acc: 0.8412 - val_loss: 0.5573 - val_acc: 0.7940\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4408 - acc: 0.8427 - val_loss: 0.5560 - val_acc: 0.7943\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4379 - acc: 0.8447 - val_loss: 0.5549 - val_acc: 0.7973\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4351 - acc: 0.8455 - val_loss: 0.5555 - val_acc: 0.7963\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4321 - acc: 0.8463 - val_loss: 0.5525 - val_acc: 0.7953\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4293 - acc: 0.8466 - val_loss: 0.5518 - val_acc: 0.7983\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4266 - acc: 0.8488 - val_loss: 0.5515 - val_acc: 0.8000\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4238 - acc: 0.8492 - val_loss: 0.5517 - val_acc: 0.7943\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4213 - acc: 0.8506 - val_loss: 0.5518 - val_acc: 0.8020\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4185 - acc: 0.8527 - val_loss: 0.5492 - val_acc: 0.7990\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4165 - acc: 0.8524 - val_loss: 0.5488 - val_acc: 0.7973\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4137 - acc: 0.8547 - val_loss: 0.5489 - val_acc: 0.7993\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4112 - acc: 0.8551 - val_loss: 0.5482 - val_acc: 0.7990\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4090 - acc: 0.8556 - val_loss: 0.5499 - val_acc: 0.7997\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4067 - acc: 0.8571 - val_loss: 0.5491 - val_acc: 0.8007\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.4045 - acc: 0.8579 - val_loss: 0.5484 - val_acc: 0.8010\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4023 - acc: 0.8585 - val_loss: 0.5475 - val_acc: 0.7977\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4000 - acc: 0.8591 - val_loss: 0.5492 - val_acc: 0.8010\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3979 - acc: 0.8605 - val_loss: 0.5478 - val_acc: 0.7997\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3957 - acc: 0.8604 - val_loss: 0.5488 - val_acc: 0.8003\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3935 - acc: 0.8611 - val_loss: 0.5498 - val_acc: 0.7993\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3919 - acc: 0.8617 - val_loss: 0.5474 - val_acc: 0.8047\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3899 - acc: 0.8633 - val_loss: 0.5478 - val_acc: 0.8033\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3878 - acc: 0.8634 - val_loss: 0.5468 - val_acc: 0.8037\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3855 - acc: 0.8644 - val_loss: 0.5474 - val_acc: 0.8030\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3838 - acc: 0.8645 - val_loss: 0.5497 - val_acc: 0.8013\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3820 - acc: 0.8656 - val_loss: 0.5475 - val_acc: 0.8050\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3802 - acc: 0.8662 - val_loss: 0.5476 - val_acc: 0.8043\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3781 - acc: 0.8681 - val_loss: 0.5476 - val_acc: 0.8050\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3763 - acc: 0.8671 - val_loss: 0.5482 - val_acc: 0.8053\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3747 - acc: 0.8681 - val_loss: 0.5486 - val_acc: 0.8050\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3726 - acc: 0.8698 - val_loss: 0.5486 - val_acc: 0.8030\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3712 - acc: 0.8701 - val_loss: 0.5481 - val_acc: 0.8057\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3697 - acc: 0.8701 - val_loss: 0.5495 - val_acc: 0.8057\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.3678 - acc: 0.8715 - val_loss: 0.5507 - val_acc: 0.8023\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.3658 - acc: 0.8715 - val_loss: 0.5490 - val_acc: 0.8080\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3646 - acc: 0.8720 - val_loss: 0.5490 - val_acc: 0.8060\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3630 - acc: 0.8740 - val_loss: 0.5498 - val_acc: 0.8077\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3614 - acc: 0.8742 - val_loss: 0.5518 - val_acc: 0.8043\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3596 - acc: 0.8739 - val_loss: 0.5513 - val_acc: 0.8050\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3579 - acc: 0.8747 - val_loss: 0.5512 - val_acc: 0.8057\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3563 - acc: 0.8749 - val_loss: 0.5514 - val_acc: 0.8063\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3552 - acc: 0.8761 - val_loss: 0.5517 - val_acc: 0.8047\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3536 - acc: 0.8770 - val_loss: 0.5527 - val_acc: 0.8057\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3521 - acc: 0.8769 - val_loss: 0.5532 - val_acc: 0.8057\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3506 - acc: 0.8781 - val_loss: 0.5538 - val_acc: 0.8070\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3490 - acc: 0.8787 - val_loss: 0.5540 - val_acc: 0.8053\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.3474 - acc: 0.8795 - val_loss: 0.5542 - val_acc: 0.8067\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3462 - acc: 0.8794 - val_loss: 0.5558 - val_acc: 0.8077\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3449 - acc: 0.8798 - val_loss: 0.5563 - val_acc: 0.8067\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3433 - acc: 0.8802 - val_loss: 0.5551 - val_acc: 0.8060\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3419 - acc: 0.8812 - val_loss: 0.5563 - val_acc: 0.8060\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3408 - acc: 0.8821 - val_loss: 0.5578 - val_acc: 0.8053\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3390 - acc: 0.8817 - val_loss: 0.5599 - val_acc: 0.8063\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3380 - acc: 0.8828 - val_loss: 0.5574 - val_acc: 0.8063\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3364 - acc: 0.8836 - val_loss: 0.5588 - val_acc: 0.8073\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3349 - acc: 0.8836 - val_loss: 0.5611 - val_acc: 0.8070\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.3338 - acc: 0.8849 - val_loss: 0.5615 - val_acc: 0.8063\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3326 - acc: 0.8850 - val_loss: 0.5629 - val_acc: 0.8060\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3316 - acc: 0.8847 - val_loss: 0.5618 - val_acc: 0.8080\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3300 - acc: 0.8854 - val_loss: 0.5626 - val_acc: 0.8070\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.3285 - acc: 0.8870 - val_loss: 0.5659 - val_acc: 0.8047\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3276 - acc: 0.8866 - val_loss: 0.5632 - val_acc: 0.8057\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3261 - acc: 0.8867 - val_loss: 0.5663 - val_acc: 0.8080\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.3252 - acc: 0.8876 - val_loss: 0.5659 - val_acc: 0.8067\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3237 - acc: 0.8882 - val_loss: 0.5654 - val_acc: 0.8067\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3227 - acc: 0.8889 - val_loss: 0.5658 - val_acc: 0.8080\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3211 - acc: 0.8891 - val_loss: 0.5671 - val_acc: 0.8040\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3198 - acc: 0.8898 - val_loss: 0.5691 - val_acc: 0.8047\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3186 - acc: 0.8908 - val_loss: 0.5697 - val_acc: 0.8073\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3173 - acc: 0.8915 - val_loss: 0.5722 - val_acc: 0.8063\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3166 - acc: 0.8908 - val_loss: 0.5699 - val_acc: 0.8063\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3151 - acc: 0.8915 - val_loss: 0.5726 - val_acc: 0.8067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3142 - acc: 0.8915 - val_loss: 0.5720 - val_acc: 0.8050\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3128 - acc: 0.8919 - val_loss: 0.5765 - val_acc: 0.8023\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3119 - acc: 0.8922 - val_loss: 0.5740 - val_acc: 0.8090\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 20us/step\n",
      "4000/4000 [==============================] - 0s 21us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30776442425178757, 0.8957575757575758]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6008252638578415, 0.805]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
